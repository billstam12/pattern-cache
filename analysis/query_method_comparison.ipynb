{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f7fa06b",
   "metadata": {},
   "source": [
    "# Query Performance Comparison Across Datasets\n",
    "\n",
    "This notebook compares query performance between different methods across all available datasets. It focuses on:\n",
    "- Comparing query execution times between standard and cached queries\n",
    "- Analyzing performance by dataset and operation type\n",
    "- Visualizing performance differences with clear and concise plots\n",
    "- Aggregating results across multiple runs for more robust comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0eff8e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "import re  # Add this import for regular expressions\n",
    "from pathlib import Path\n",
    "import scipy.stats as stats\n",
    "import warnings\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set publication-ready plotting style\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'serif',\n",
    "    'font.serif': ['Times', 'Times New Roman', 'Palatino', 'DejaVu Serif'],\n",
    "    'font.size': 12,\n",
    "    'axes.titlesize': 18,\n",
    "    'axes.labelsize': 18,\n",
    "    'xtick.labelsize': 16,\n",
    "    'ytick.labelsize': 16,\n",
    "    'legend.fontsize': 14,\n",
    "    'figure.figsize': [10, 6],\n",
    "    'figure.dpi': 150,\n",
    "    'savefig.dpi': 300,\n",
    "    'savefig.bbox': 'tight',\n",
    "    'savefig.pad_inches': 0.05,\n",
    "    'axes.grid': True,\n",
    "    'grid.alpha': 0.3,\n",
    "    'axes.axisbelow': True})\n",
    "\n",
    "# Define output folders\n",
    "outFolder = \"output_no_allocation\"\n",
    "groundTruthFolder = \"output_no_allocation\"\n",
    "\n",
    "plt.rcParams['text.usetex'] = False  # Set to True only if you have LaTeX installed\n",
    "plt.rcParams['mathtext.default'] = 'regular'\n",
    "\n",
    "# Define the methods to compare\n",
    "METHODS = [\n",
    "    {\n",
    "        \"name\": \"M4-NoC\",\n",
    "        \"path\": f\"../{groundTruthFolder}/timeQueries/\",\n",
    "        \"method\": \"m4\",\n",
    "        \"patternMethod\": \"m4\",\n",
    "        \"database\": \"influx\"\n",
    "    },\n",
    "    # {\n",
    "    #     \"name\": \"M4-C\",\n",
    "    #     \"path\": f\"../{outFolder}/timeCacheQueries/\",\n",
    "    #     \"method\": \"m4\",\n",
    "    #     \"patternMethod\": \"m4\",\n",
    "    #     \"database\": \"influx\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"name\": \"M4$^\\\\infty$-C\",\n",
    "    #     \"path\": f\"../{outFolder}/timeCacheQueries/\",\n",
    "    #     \"method\": \"m4Inf\",\n",
    "    #     \"patternMethod\": \"m4Inf\",\n",
    "    #     \"database\": \"influx\"\n",
    "    # },\n",
    "    {\n",
    "        \"name\": \"M2$^\\\\infty$-C\",\n",
    "        \"path\": f\"../{outFolder}/timeCacheQueries/\",\n",
    "        \"method\": \"minmax\",\n",
    "        \"patternMethod\": \"minmax\",\n",
    "        \"database\": \"influx\"\n",
    "    },\n",
    "    # {\n",
    "    #     \"name\": \"MinMaxCache\",\n",
    "    #     \"path\": f\"../{outFolder}/timeMinMaxCacheQueries/\",\n",
    "    #     \"method\": \"minmax\",\n",
    "    #     \"patternMethod\": \"minmaxcache\",\n",
    "    #     \"database\": \"influx\"\n",
    "    # },\n",
    "]\n",
    "\n",
    "# Create a folder for saving publication-ready figures\n",
    "FIGURES_DIR = \"../figures\"\n",
    "os.makedirs(FIGURES_DIR, exist_ok=True)\n",
    "\n",
    "# Generate a publication-ready color palette\n",
    "# Using ColorBrewer-inspired palette for better distinction in papers\n",
    "# METHOD_COLORS = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
    "\n",
    "# Create a consistent color mapping function\n",
    "def get_method_color_mapping(methods_list):\n",
    "    \"\"\"\n",
    "    Create a consistent color mapping for all methods used throughout the notebook.\n",
    "    Ensures each method gets the same color in all plots.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    methods_list : list\n",
    "        List of method names or method dictionaries\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict: Mapping from method name to color\n",
    "    \"\"\"\n",
    "    color_mapping = {}\n",
    "    \n",
    "    # Extract method names if dealing with method dictionaries\n",
    "    if isinstance(methods_list[0], dict):\n",
    "        method_names = [method['name'] for method in methods_list]\n",
    "    else:\n",
    "        method_names = methods_list\n",
    "    \n",
    "    # Define specific colors for known methods to ensure consistency\n",
    "    predefined_colors = {\n",
    "        'M4-NoC': '#1f77b4',        # Blue\n",
    "        'M4-C': '#ff7f0e',          # Orange  \n",
    "        'M4$^\\\\infty$-C': '#2ca02c', # Green\n",
    "        'MinMaxCache': '#d62728',   # Red\n",
    "        'M2$^\\\\infty$-C': '#9467bd', # Purple\n",
    "    }\n",
    "    \n",
    "    # Assign predefined colors first\n",
    "    color_idx = 0\n",
    "    for method_name in method_names:\n",
    "        if method_name in predefined_colors:\n",
    "            color_mapping[method_name] = predefined_colors[method_name]\n",
    "    return color_mapping\n",
    "\n",
    "# Create global color mapping for all methods\n",
    "GLOBAL_METHOD_COLORS = get_method_color_mapping(METHODS)\n",
    "\n",
    "# Define a consistent style function for publication-ready plots\n",
    "def set_publication_style(ax, title=None, xlabel=None, ylabel=None, legend_title=None):\n",
    "    \"\"\"Apply consistent publication-ready styling to matplotlib axis\"\"\"\n",
    "    if title:\n",
    "        ax.set_title(title, fontweight='bold')\n",
    "    if xlabel:\n",
    "        ax.set_xlabel(xlabel, fontweight='bold')\n",
    "    if ylabel:\n",
    "        ax.set_ylabel(ylabel, fontweight='bold')\n",
    "    \n",
    "    # Apply grid style\n",
    "    ax.grid(True, linestyle='--', alpha=0.3)\n",
    "    ax.set_axisbelow(True)\n",
    "    \n",
    "    # Style spines\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_linewidth(1.5)\n",
    "\n",
    "    # Format legend if it exists\n",
    "    if ax.get_legend():\n",
    "        if legend_title:\n",
    "            ax.legend(title=legend_title, frameon=True, facecolor='white', \n",
    "                     framealpha=0.9, edgecolor='black')    \n",
    "    # Remove top and right spines\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8f55817b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis cache directory: ./analysis_cache\n"
     ]
    }
   ],
   "source": [
    "# Cache management setup\n",
    "import pickle\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "# Define cache directory relative to notebook location\n",
    "CACHE_DIR = \"./analysis_cache\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Analysis cache directory: {CACHE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd34fd1c",
   "metadata": {},
   "source": [
    "## Cache Management\n",
    "\n",
    "This notebook uses caching to avoid re-running expensive computations. The workflow is:\n",
    "\n",
    "1. **Calculation cells**: Load data, perform computations, and cache results\n",
    "2. **Plotting cells**: Load cached data and generate visualizations\n",
    "3. **Cache management**: Clear, inspect, or manage cached data\n",
    "\n",
    "**Cache files:**\n",
    "- `query_results.pkl`: Aggregated query performance data\n",
    "- `ssim_data.pkl`: SSIM calculation results  \n",
    "- `ssim_operation_data.pkl`: SSIM analysis by operation type\n",
    "\n",
    "Run the cache management cell below to inspect or clear cached data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "11ec4ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Cache Status ===\n",
      "Query performance data (query_results.pkl): Not cached\n",
      "SSIM calculation results (ssim_data.pkl): Not cached\n",
      "SSIM analysis by operation (ssim_operation_data.pkl): Not cached\n"
     ]
    }
   ],
   "source": [
    "# Cache Management Utilities\n",
    "\n",
    "def clear_analysis_cache():\n",
    "    \"\"\"Clear all cached analysis data\"\"\"\n",
    "    if os.path.exists(CACHE_DIR):\n",
    "        shutil.rmtree(CACHE_DIR)\n",
    "        os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "        print(f\"Cleared analysis cache directory: {CACHE_DIR}\")\n",
    "    else:\n",
    "        print(\"Cache directory does not exist\")\n",
    "\n",
    "def list_cache_contents():\n",
    "    \"\"\"List contents of the cache directory\"\"\"\n",
    "    if os.path.exists(CACHE_DIR):\n",
    "        cache_files = os.listdir(CACHE_DIR)\n",
    "        if cache_files:\n",
    "            print(f\"Cache directory contents ({CACHE_DIR}):\")\n",
    "            for file in cache_files:\n",
    "                file_path = os.path.join(CACHE_DIR, file)\n",
    "                size = os.path.getsize(file_path)\n",
    "                mod_time = os.path.getmtime(file_path)\n",
    "                mod_time_str = datetime.fromtimestamp(mod_time).strftime('%Y-%m-%d %H:%M:%S')\n",
    "                print(f\"  - {file} ({size:,} bytes, modified: {mod_time_str})\")\n",
    "        else:\n",
    "            print(\"Cache directory is empty\")\n",
    "    else:\n",
    "        print(\"Cache directory does not exist\")\n",
    "\n",
    "def get_cache_info():\n",
    "    \"\"\"Get information about cached data files\"\"\"\n",
    "    cache_files = {\n",
    "        'query_results.pkl': 'Query performance data',\n",
    "        'ssim_data.pkl': 'SSIM calculation results',\n",
    "        'ssim_operation_data.pkl': 'SSIM analysis by operation'\n",
    "    }\n",
    "    \n",
    "    for filename, description in cache_files.items():\n",
    "        filepath = os.path.join(CACHE_DIR, filename)\n",
    "        if os.path.exists(filepath):\n",
    "            size = os.path.getsize(filepath)\n",
    "            mod_time = os.path.getmtime(filepath)\n",
    "            mod_time_str = datetime.fromtimestamp(mod_time).strftime('%Y-%m-%d %H:%M:%S')\n",
    "            \n",
    "            print(f\"{description} ({filename}):\")\n",
    "            print(f\"  - Size: {size:,} bytes\")\n",
    "            print(f\"  - Modified: {mod_time_str}\")\n",
    "            \n",
    "            # Try to load and show basic info\n",
    "            try:\n",
    "                with open(filepath, 'rb') as f:\n",
    "                    cached_data = pickle.load(f)\n",
    "                \n",
    "                if filename == 'query_results.pkl':\n",
    "                    if 'all_combined' in cached_data and cached_data['all_combined'] is not None:\n",
    "                        df = cached_data['all_combined']\n",
    "                        print(f\"  - Records: {len(df)}\")\n",
    "                        print(f\"  - Datasets: {', '.join(df['dataset'].unique())}\")\n",
    "                        print(f\"  - Methods: {', '.join(df['method'].unique())}\")\n",
    "                \n",
    "                elif filename == 'ssim_data.pkl':\n",
    "                    if 'ssim_df' in cached_data and cached_data['ssim_df'] is not None:\n",
    "                        df = cached_data['ssim_df']\n",
    "                        print(f\"  - Records: {len(df)}\")\n",
    "                        print(f\"  - Datasets: {', '.join(df['dataset'].unique())}\")\n",
    "                        print(f\"  - Methods: {', '.join(df['method'].unique())}\")\n",
    "                \n",
    "                elif filename == 'ssim_operation_data.pkl':\n",
    "                    if 'ssim_with_ops' in cached_data and cached_data['ssim_with_ops'] is not None:\n",
    "                        df = cached_data['ssim_with_ops']\n",
    "                        print(f\"  - Records: {len(df)}\")\n",
    "                        print(f\"  - Operations: {', '.join(df['operation'].unique())}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  - Error reading cache: {str(e)}\")\n",
    "            print()\n",
    "        else:\n",
    "            print(f\"{description} ({filename}): Not cached\")\n",
    "\n",
    "# Uncomment the line below to clear all cache and force recalculation\n",
    "# clear_analysis_cache()\n",
    "\n",
    "# Show current cache status\n",
    "print(\"=== Cache Status ===\")\n",
    "get_cache_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836d9f9c",
   "metadata": {},
   "source": [
    "## Load Query Results Data\n",
    "\n",
    "Load experiment results from all available datasets, aggregating across runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a5995cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_results(base_path, method, database_type, table_name):\n",
    "    \"\"\"\n",
    "    Load results from multiple experiment runs into a single dataframe\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    base_path : str\n",
    "        Base path to the queries directory\n",
    "    method : str    \n",
    "        Name of the method used (e.g., m4Inf, m4)\n",
    "    database_type : str\n",
    "        Type of database (influx, postgres, etc.)\n",
    "    table_name : str\n",
    "        Name of the database table\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame or None: Combined results from all runs\n",
    "    \"\"\"\n",
    "    path_pattern = os.path.join(base_path, method, database_type, table_name, \"run_*\", \"results.csv\")\n",
    "    csv_files = glob.glob(path_pattern)\n",
    "    \n",
    "    if not csv_files:\n",
    "        return None\n",
    "    \n",
    "    dfs = []\n",
    "    for csv_file in csv_files:\n",
    "        run_name = os.path.basename(os.path.dirname(csv_file))\n",
    "        df = pd.read_csv(csv_file)\n",
    "        df['run'] = run_name\n",
    "        df['dataset'] = table_name\n",
    "        dfs.append(df)\n",
    "    \n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Convert date columns to datetime if they exist\n",
    "    date_columns = ['from', 'to']\n",
    "    for col in date_columns:\n",
    "        if col in combined_df.columns:\n",
    "            combined_df[col] = pd.to_datetime(combined_df[col])\n",
    "            \n",
    "    # Add duration column\n",
    "    if 'from' in combined_df.columns and 'to' in combined_df.columns:\n",
    "        combined_df['duration_sec'] = (combined_df['to'] - combined_df['from']).dt.total_seconds()\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "# Function to detect and filter out outlier runs\n",
    "def filter_outlier_runs(df, outlier_threshold=2.0):\n",
    "    \"\"\"\n",
    "    Filter out runs that have significantly different execution times compared to other runs\n",
    "    for the same query using the Interquartile Range (IQR) method.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame containing results from multiple runs\n",
    "    outlier_threshold : float\n",
    "        IQR multiplier for outlier detection (default: 2.0 for more conservative filtering)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame: DataFrame with outlier runs removed\n",
    "    \"\"\"\n",
    "    if df is None or len(df) == 0:\n",
    "        return df\n",
    "    \n",
    "    # Group by query characteristics to identify the same query across runs\n",
    "    groupby_cols = ['dataset', 'query #', 'query_type', 'group_by', 'aggregation', 'time_interval']\n",
    "    group_cols = [col for col in groupby_cols if col in df.columns]\n",
    "    \n",
    "    filtered_dfs = []\n",
    "    outliers_removed = 0\n",
    "    \n",
    "    for group_key, group_df in df.groupby(group_cols):\n",
    "        if len(group_df) <= 2:  # Don't filter if we have 2 or fewer runs\n",
    "            filtered_dfs.append(group_df)\n",
    "            continue\n",
    "        \n",
    "        # Calculate IQR for execution times\n",
    "        times = group_df['Time (sec)']\n",
    "        Q1 = times.quantile(0.25)\n",
    "        Q3 = times.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        # Define outlier bounds\n",
    "        lower_bound = Q1 - outlier_threshold * IQR\n",
    "        upper_bound = Q3 + outlier_threshold * IQR\n",
    "        \n",
    "        # Filter out outliers\n",
    "        mask = (times >= lower_bound) & (times <= upper_bound)\n",
    "        filtered_group = group_df[mask]\n",
    "        \n",
    "        # Only apply filtering if we still have at least 2 runs after filtering\n",
    "        if len(filtered_group) >= 2:\n",
    "            outliers_removed += len(group_df) - len(filtered_group)\n",
    "            filtered_dfs.append(filtered_group)\n",
    "        else:\n",
    "            # Keep original data if filtering would leave us with too few runs\n",
    "            filtered_dfs.append(group_df)\n",
    "    \n",
    "    if filtered_dfs:\n",
    "        result_df = pd.concat(filtered_dfs, ignore_index=True)\n",
    "        if outliers_removed > 0:\n",
    "            print(f\"    Filtered out {outliers_removed} outlier runs\")\n",
    "        return result_df\n",
    "    else:\n",
    "        return df\n",
    "\n",
    "# Function to aggregate results from multiple runs\n",
    "def aggregate_runs(df):\n",
    "    \"\"\"\n",
    "    Aggregate results from multiple runs by grouping by query characteristics\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame containing results from multiple runs\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame: Aggregated results with statistics across runs\n",
    "    \"\"\"\n",
    "    if df is None or len(df) == 0:\n",
    "        return None\n",
    "    \n",
    "    # First, filter out outlier runs\n",
    "    df_filtered = filter_outlier_runs(df)\n",
    "    \n",
    "    # Group by query characteristics (not by run)\n",
    "    groupby_cols = ['dataset', 'query #', 'query_type', 'group_by', 'aggregation', 'time_interval']\n",
    "    group_cols = [col for col in groupby_cols if col in df_filtered.columns]\n",
    "    \n",
    "    # Aggregate the Time (sec) column across runs\n",
    "    agg_df = df_filtered.groupby(group_cols).agg({\n",
    "        'Time (sec)': ['mean', 'median', 'std', 'min', 'max', 'count'],\n",
    "        'run': 'nunique'  # Count number of runs\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Flatten the multi-level column names\n",
    "    agg_df.columns = [f\"{col[0]}_{col[1]}\" if col[1] else col[0] for col in agg_df.columns]\n",
    "    \n",
    "    # Rename some columns for clarity\n",
    "    agg_df = agg_df.rename(columns={\n",
    "        'Time (sec)_mean': 'Time (sec)', \n",
    "        'Time (sec)_count': 'query_count',\n",
    "        'run_nunique': 'run_count'\n",
    "    })\n",
    "    \n",
    "    return agg_df\n",
    "\n",
    "# Operation type mapping for better readability\n",
    "def get_operation_type_mapping():\n",
    "    return {\n",
    "        'P': 'Pan',\n",
    "        'ZI': 'Zoom In',\n",
    "        'ZO': 'Zoom Out',\n",
    "        'R': 'Resize',\n",
    "        'MC': 'Measure Change',\n",
    "        'PD': 'Pattern Detection',\n",
    "        'NaN': 'Initial Query'\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de14474c",
   "metadata": {},
   "source": [
    "## Find All Available Datasets and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a2b30cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing query results from scratch...\n",
      "Found 0 datasets: []\n",
      "\n",
      "No data was loaded.\n",
      "\n",
      "Cached query results to ./analysis_cache/query_results.pkl\n"
     ]
    }
   ],
   "source": [
    "# Query Data Loading and Aggregation - CALCULATION CELL\n",
    "# This cell loads raw query data, performs aggregation, and caches the results\n",
    "\n",
    "def find_datasets(method_info):\n",
    "    \"\"\"Find all available datasets by looking at directories\"\"\"\n",
    "    base_path = method_info[\"path\"]\n",
    "    method = method_info[\"method\"]\n",
    "    database = method_info[\"database\"]\n",
    "    \n",
    "    # Find all dataset directories\n",
    "    dataset_pattern = os.path.join(base_path, method, database, \"*\")\n",
    "    datasets = []\n",
    "    \n",
    "    for dataset_dir in glob.glob(dataset_pattern):\n",
    "        if os.path.isdir(dataset_dir):\n",
    "            dataset_name = os.path.basename(dataset_dir)\n",
    "            datasets.append(dataset_name)\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "# Check if query results are already cached\n",
    "query_cache_file = os.path.join(CACHE_DIR, \"query_results.pkl\")\n",
    "\n",
    "if os.path.exists(query_cache_file):\n",
    "    print(\"Loading cached query results...\")\n",
    "    with open(query_cache_file, 'rb') as f:\n",
    "        cached_data = pickle.load(f)\n",
    "        raw_results_by_dataset = cached_data.get('raw_results_by_dataset', {})\n",
    "        results_by_dataset = cached_data.get('results_by_dataset', {})\n",
    "        all_combined = cached_data.get('all_combined')\n",
    "        all_datasets = cached_data.get('all_datasets', [])\n",
    "    \n",
    "    if all_combined is not None:\n",
    "        print(f\"Loaded cached data: {len(all_combined)} aggregated queries across {len(all_datasets)} datasets\")\n",
    "        print(f\"Datasets: {', '.join(all_datasets)}\")\n",
    "        print(f\"Methods: {', '.join(all_combined['method'].unique())}\")\n",
    "    else:\n",
    "        print(\"Warning: Cached data appears to be empty\")\n",
    "else:\n",
    "    print(\"Computing query results from scratch...\")\n",
    "    \n",
    "    # Get unique datasets from all methods\n",
    "    all_datasets = set()\n",
    "    for method in METHODS:\n",
    "        datasets = find_datasets(method)\n",
    "        all_datasets.update(datasets)\n",
    "\n",
    "    all_datasets = sorted(list(all_datasets))\n",
    "    print(f\"Found {len(all_datasets)} datasets: {all_datasets}\")\n",
    "\n",
    "    # Load data for each method and dataset\n",
    "    raw_results_by_dataset = {}  # Store raw results\n",
    "    results_by_dataset = {}      # Store aggregated results\n",
    "    all_results = []             # Store all aggregated results\n",
    "\n",
    "    for dataset in all_datasets:\n",
    "        print(f\"\\nLoading data for dataset: {dataset}\")\n",
    "        dataset_results = []\n",
    "        dataset_raw_results = []\n",
    "        \n",
    "        for i, method in enumerate(METHODS):\n",
    "            print(f\"  Loading {method['name']}...\")\n",
    "            \n",
    "            df = load_results(\n",
    "                base_path=method['path'],\n",
    "                method=method['method'],\n",
    "                database_type=method['database'],\n",
    "                table_name=dataset\n",
    "            )\n",
    "            \n",
    "            if df is not None and not df.empty:\n",
    "                # Add method name and color index\n",
    "                df['method'] = method['name']\n",
    "                df['method_idx'] = i\n",
    "                \n",
    "                # Add readable operation type\n",
    "                op_type_map = get_operation_type_mapping()\n",
    "                df['operation'] = df.apply(\n",
    "                    lambda row: 'Initial Query' if pd.isna(row['query_type']) else op_type_map.get(row['query_type'], row['query_type']), \n",
    "                    axis=1\n",
    "                )\n",
    "                \n",
    "                # Store raw results first\n",
    "                dataset_raw_results.append(df)\n",
    "                \n",
    "                # Aggregate results across runs\n",
    "                agg_df = aggregate_runs(df)\n",
    "                if agg_df is not None:\n",
    "                    # Add method name and operation type to aggregated data\n",
    "                    agg_df['method'] = method['name']\n",
    "                    agg_df['method_idx'] = i\n",
    "                    agg_df['operation'] = agg_df.apply(\n",
    "                        lambda row: 'Initial Query' if pd.isna(row['query_type']) else op_type_map.get(row['query_type'], row['query_type']), \n",
    "                        axis=1\n",
    "                    )\n",
    "                    \n",
    "                    dataset_results.append(agg_df)\n",
    "                    all_results.append(agg_df)\n",
    "                    print(f\"    Loaded {len(df)} queries from {agg_df['run_count'].iloc[0]} runs, aggregated to {len(agg_df)} unique queries\")\n",
    "                else:\n",
    "                    print(f\"    Error aggregating results\")\n",
    "            else:\n",
    "                print(f\"    No data found\")\n",
    "        \n",
    "        if dataset_raw_results:\n",
    "            raw_results_by_dataset[dataset] = pd.concat(dataset_raw_results, ignore_index=True)\n",
    "        \n",
    "        if dataset_results:\n",
    "            results_by_dataset[dataset] = pd.concat(dataset_results, ignore_index=True)\n",
    "\n",
    "    # Combine all results into a single dataframe for overall analysis\n",
    "    if all_results:\n",
    "        all_combined = pd.concat(all_results, ignore_index=True)\n",
    "        print(f\"\\nLoaded a total of {len(all_combined)} aggregated queries across {len(all_datasets)} datasets\")\n",
    "        \n",
    "        # Print run count information\n",
    "        for dataset in results_by_dataset:\n",
    "            for method in METHODS:\n",
    "                method_name = method['name']\n",
    "                method_data = results_by_dataset[dataset][results_by_dataset[dataset]['method'] == method_name]\n",
    "                if not method_data.empty:\n",
    "                    run_count = method_data['run_count'].iloc[0]\n",
    "                    print(f\"Dataset: {dataset}, Method: {method_name}, Runs: {run_count}\")\n",
    "    else:\n",
    "        all_combined = None\n",
    "        print(\"\\nNo data was loaded.\")\n",
    "    \n",
    "    # Cache the results\n",
    "    cache_data = {\n",
    "        'raw_results_by_dataset': raw_results_by_dataset,\n",
    "        'results_by_dataset': results_by_dataset,\n",
    "        'all_combined': all_combined,\n",
    "        'all_datasets': all_datasets\n",
    "    }\n",
    "    \n",
    "    # with open(query_cache_file, 'wb') as f:\n",
    "        # pickle.dump(cache_data, f)\n",
    "    print(f\"\\nCached query results to {query_cache_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05cdfaf",
   "metadata": {},
   "source": [
    "## Performance Comparison by Operation Type\n",
    "\n",
    "Let's break down the performance by operation type for each dataset using the aggregated results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ab0ee8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data available for comparison.\n"
     ]
    }
   ],
   "source": [
    "# Performance Comparison by Operation Type - PLOTTING CELL\n",
    "# This cell loads cached query data and generates operation-type performance visualizations\n",
    "\n",
    "if 'all_combined' in globals() and all_combined is not None and 'results_by_dataset' in globals():\n",
    "    # Get unique operations across all datasets\n",
    "    all_ops = sorted(all_combined['operation'].unique())\n",
    "    \n",
    "    # Create publication-ready figures for each dataset separately\n",
    "    for i, dataset in enumerate(all_datasets):\n",
    "        if dataset in results_by_dataset:\n",
    "            dataset_df = results_by_dataset[dataset]\n",
    "            method_names = dataset_df['method'].unique()\n",
    "            run_count = dataset_df['run_count'].iloc[0]\n",
    "            \n",
    "            # Group data by operation type and method\n",
    "            op_perf = dataset_df.groupby(['operation', 'method'])['Time (sec)'].mean().reset_index()\n",
    "            \n",
    "            # Remove rows with NaN operations to avoid plotting issues\n",
    "            op_perf = op_perf.dropna(subset=['operation'])\n",
    "            \n",
    "            # Get operations for this dataset and sort them in a meaningful order\n",
    "            operations = op_perf['operation'].unique()\n",
    "            op_order = ['Initial Query', 'Pan', 'Zoom In', 'Zoom Out', 'Resize', 'Measure Change', 'Pattern Detection']\n",
    "            operations = sorted(operations, key=lambda x: op_order.index(x) if x in op_order else 999)\n",
    "            \n",
    "            # Create the figure\n",
    "            fig, ax = plt.subplots(figsize=(12, 7))\n",
    "            \n",
    "            # Set bar properties\n",
    "            bar_width = 0.6 / len(method_names)\n",
    "            opacity = 0.8\n",
    "            bar_positions = np.arange(len(operations))\n",
    "            \n",
    "            method_handles = []  # Store handles for legend\n",
    "            \n",
    "            for j, method in enumerate(method_names):\n",
    "                method_data = op_perf[op_perf['method'] == method]\n",
    "                # Create a lookup dict by operation\n",
    "                method_by_op = {row['operation']: row['Time (sec)'] for _, row in method_data.iterrows()}\n",
    "                \n",
    "                # Extract values in the correct order\n",
    "                values = [method_by_op.get(op, 0) for op in operations]\n",
    "                \n",
    "                # Plot bars with consistent colors\n",
    "                offset = (j - len(method_names)/2 + 0.5) * bar_width\n",
    "                bars = ax.bar(\n",
    "                    bar_positions + offset, \n",
    "                    values, \n",
    "                    bar_width,\n",
    "                    color=GLOBAL_METHOD_COLORS[method], \n",
    "                    label=method,\n",
    "                    edgecolor='black',\n",
    "                    linewidth=1,\n",
    "                    alpha=opacity\n",
    "                )\n",
    "                \n",
    "                # Add value labels on top of bars\n",
    "                for k, bar in enumerate(bars):\n",
    "                    height = bar.get_height()\n",
    "                    if height > 0:  # Only add labels for non-zero values\n",
    "                        ax.text(\n",
    "                            bar.get_x() + bar.get_width()/2,\n",
    "                            height + 0.05,\n",
    "                            f'{height:.2f}',\n",
    "                            ha='center', \n",
    "                            va='bottom',\n",
    "                            rotation=0\n",
    "                        )\n",
    "            \n",
    "            # Set the x-axis labels\n",
    "            ax.set_xticks(bar_positions)\n",
    "            ax.set_xticklabels(operations, rotation=45, ha='right')\n",
    "            \n",
    "            # Set labels and title\n",
    "            title = f'Query Performance by Operation Type - {dataset}'\n",
    "            subtitle = f'Average across {run_count} runs'\n",
    "            ax.set_title(f'{title}\\n{subtitle}', pad=20)\n",
    "            ax.set_xlabel('Operation Type')\n",
    "            ax.set_ylabel('Average Time (seconds)')\n",
    "            # Create legend with pattern detection highlight (only once)\n",
    "            legend_handles = method_handles\n",
    "            \n",
    "            # Create legend with custom handles\n",
    "            ax.legend(handles=legend_handles, title=\"Query Method\", loc='upper right')\n",
    "                \n",
    "            # Apply publication styling (legend will be created automatically from labels)\n",
    "            set_publication_style(ax, legend_title='Query Method')\n",
    "            \n",
    "            # Adjust layout\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save figure\n",
    "            filename_safe = dataset.replace('/', '_').replace(' ', '_')\n",
    "            plt.savefig(os.path.join(FIGURES_DIR, f\"operations_{filename_safe}.pdf\"))\n",
    "            plt.savefig(os.path.join(FIGURES_DIR, f\"operations_{filename_safe}.png\"))\n",
    "            plt.show()\n",
    "else:\n",
    "    print(\"No data available for comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa46955",
   "metadata": {},
   "source": [
    "## Query Execution Time Evolution\n",
    "\n",
    "Let's visualize how query times evolve across the sequence of operations, highlighting pattern detection queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5ff42579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data available for plotting time evolution.\n"
     ]
    }
   ],
   "source": [
    "if all_combined is not None:\n",
    "    # Plot time series for each dataset\n",
    "    for dataset in all_datasets:\n",
    "        if dataset in results_by_dataset:\n",
    "            dataset_df = results_by_dataset[dataset]\n",
    "            run_count = dataset_df['run_count'].iloc[0]\n",
    "            \n",
    "            # Create a publication-ready time series plot\n",
    "            fig, ax = plt.subplots(figsize=(12, 7))\n",
    "            \n",
    "            methods = dataset_df['method'].unique()\n",
    "            \n",
    "            # Define markers and line styles by operation type\n",
    "            markers = {'Pattern Detection': '*', 'Other': 'o'}\n",
    "            \n",
    "            # Create a variable to track the max y-value for annotation positioning\n",
    "            max_y = 0\n",
    "            \n",
    "            # Add highlighting for pattern detection queries\n",
    "            pattern_queries = dataset_df[dataset_df['operation'] == 'Pattern Detection']['query #'].unique()\n",
    "            for query_num in pattern_queries:\n",
    "                ax.axvline(x=query_num, color='lightgray', linestyle='--', alpha=0.5, zorder=0)\n",
    "\n",
    "            # Plot each method\n",
    "            method_handles = []\n",
    "            highlight_handle = None\n",
    "            for i, method in enumerate(methods):\n",
    "                method_data = dataset_df[dataset_df['method'] == method].sort_values('query #')\n",
    "                pd_data = method_data[method_data['operation'] == 'Pattern Detection'].sort_values('query #')\n",
    "\n",
    "                # Use consistent color for this method\n",
    "                color = GLOBAL_METHOD_COLORS[method]\n",
    "                \n",
    "                # Plot standard queries with error bars\n",
    "                line = ax.errorbar(\n",
    "                    method_data['query #'], \n",
    "                    method_data['Time (sec)'],\n",
    "                    yerr=method_data['Time (sec)_std'],\n",
    "                    label=method,\n",
    "                    marker=markers['Other'], \n",
    "                    markersize=7, \n",
    "                    alpha=0.9,\n",
    "                    color=color, \n",
    "                    linestyle='-', \n",
    "                    linewidth=2,\n",
    "                    capsize=4,\n",
    "                    capthick=1,\n",
    "                    elinewidth=1\n",
    "                )\n",
    "                method_handles.append(line)\n",
    "\n",
    "                sz = 0.05                                \n",
    "           \n",
    "            # Set proper titles and labels\n",
    "            title = f'Query Execution Time Evolution - {dataset}'\n",
    "            subtitle = f'Average of {run_count} runs with standard deviation'\n",
    "            ax.set_title(f'{title}\\n{subtitle}', pad=20)\n",
    "            ax.set_xlabel('Query Sequence Number')\n",
    "            ax.set_ylabel('Execution Time (seconds)')\n",
    "            \n",
    "            # Format x-axis as integers\n",
    "            ax.xaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n",
    "            \n",
    "            # Create legend with pattern detection highlight (only once)\n",
    "            legend_handles = method_handles\n",
    "            if highlight_handle is not None:  # Only add if pattern queries exist\n",
    "                legend_handles.append(highlight_handle)\n",
    "            \n",
    "            # Create legend with custom handles\n",
    "            ax.legend(handles=legend_handles, title=\"Query Method / Highlight\", loc='upper right')\n",
    "            \n",
    "            # Apply publication styling\n",
    "            set_publication_style(ax, legend_title='Query Method')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save figure\n",
    "            filename_safe = dataset.replace(' ', '_').lower()\n",
    "            plt.savefig(os.path.join(FIGURES_DIR, f\"time_evolution_{filename_safe}.pdf\"))\n",
    "            plt.savefig(os.path.join(FIGURES_DIR, f\"time_evolution_{filename_safe}.png\"))\n",
    "            plt.show()\n",
    "else:\n",
    "    print(\"No data available for plotting time evolution.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0618a68f",
   "metadata": {},
   "source": [
    "## Performance Distribution Analysis\n",
    "\n",
    "Compare the distribution of query times between methods for each dataset using aggregated statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "95042bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data available for distribution analysis.\n"
     ]
    }
   ],
   "source": [
    "if all_combined is not None:\n",
    "   \n",
    "    # We'll use raw_results_by_dataset to get the minimum value for each query\n",
    "    for dataset in all_datasets:\n",
    "        if dataset in raw_results_by_dataset:\n",
    "            dataset_df = raw_results_by_dataset[dataset]\n",
    "            run_count = len(dataset_df['run'].unique())\n",
    "            \n",
    "            # Create publication-ready chart for performance distribution\n",
    "            fig, ax = plt.subplots(figsize=(10, 7))\n",
    "            \n",
    "            methods = dataset_df['method'].unique()\n",
    "            positions = np.arange(len(methods))\n",
    "            width = 0.6\n",
    "            \n",
    "            # Create a more sophisticated boxplot-like visualization using minimum times\n",
    "            for i, method in enumerate(methods):\n",
    "                # Get data for this method\n",
    "                method_data = dataset_df[dataset_df['method'] == method]\n",
    "                \n",
    "                # Calculate the minimum time for each unique query across all runs\n",
    "                min_times_by_query = method_data.groupby(['query #'])['Time (sec)'].min().reset_index()\n",
    "                \n",
    "                # Calculate statistics on these minimum times\n",
    "                mean_val = min_times_by_query['Time (sec)'].mean()\n",
    "                median_val = min_times_by_query['Time (sec)'].median()\n",
    "                min_val = min_times_by_query['Time (sec)'].min()\n",
    "                max_val = min_times_by_query['Time (sec)'].max()\n",
    "                std_val = min_times_by_query['Time (sec)'].std()\n",
    "                \n",
    "                # Draw box - Ensure box doesn't go below zero\n",
    "                box_bottom = max(0, median_val - std_val/2)  # Use max() to prevent negative values\n",
    "                box_height = std_val\n",
    "                box = plt.Rectangle(\n",
    "                    (i-width/2, box_bottom),\n",
    "                    width, box_height,\n",
    "                    alpha=0.7,\n",
    "                    facecolor=GLOBAL_METHOD_COLORS[method],\n",
    "                    edgecolor='black',\n",
    "                    linewidth=1.5\n",
    "                )\n",
    "                ax.add_patch(box)\n",
    "                \n",
    "                # Draw median line\n",
    "                ax.plot([i-width/2, i+width/2], [median_val, median_val], \n",
    "                        color='white', linewidth=2.5, solid_capstyle='round')\n",
    "                ax.plot([i-width/2, i+width/2], [median_val, median_val], \n",
    "                        color='black', linewidth=1.5, solid_capstyle='round')\n",
    "                \n",
    "                # Draw whiskers (min-max) - Ensure whiskers don't go below zero\n",
    "                ax.plot([i, i], [max(0, min_val), box_bottom], \n",
    "                        color='black', linewidth=1.5, linestyle='-')\n",
    "                ax.plot([i, i], [box_bottom + box_height, max_val], \n",
    "                        color='black', linewidth=1.5, linestyle='-')\n",
    "                \n",
    "                # Draw caps on whiskers\n",
    "                whisker_width = width / 4\n",
    "                ax.plot([i-whisker_width, i+whisker_width], [max(0, min_val), max(0, min_val)], \n",
    "                        color='black', linewidth=1.5)\n",
    "                ax.plot([i-whisker_width, i+whisker_width], [max_val, max_val], \n",
    "                        color='black', linewidth=1.5)\n",
    "                \n",
    "                # Draw mean point\n",
    "                ax.plot(i, mean_val, 'o', color='white', markersize=8)\n",
    "                ax.plot(i, mean_val, 'o', color='black', markersize=6)\n",
    "                \n",
    "                # Add annotation with statistics\n",
    "                stats_text = (\n",
    "                    f\"n={len(min_times_by_query)}\\n\"\n",
    "                    f\"mean={mean_val:.2f}s\\n\"\n",
    "                    f\"median={median_val:.2f}s\\n\"\n",
    "                    f\"std={std_val:.2f}\\n\"\n",
    "                    f\"min={min_val:.2f}s\\n\"\n",
    "                    f\"max={max_val:.2f}s\"\n",
    "                )\n",
    "                \n",
    "                # Improved stats text positioning strategy\n",
    "                text_x = i + width * 0.7  # Position to the right side of the box\n",
    "                text_y = (box_bottom + box_height + max_val) / 2  # Middle between box top and max whisker\n",
    "                \n",
    "                # Alternative positioning based on dataset characteristics\n",
    "                if max_val > mean_val * 3:  # If we have extreme outliers\n",
    "                    text_y = median_val + std_val  # Place near the upper part of the box\n",
    "                \n",
    "                # Ensure text is always inside the plot area\n",
    "                y_min, y_max = ax.get_ylim()\n",
    "                text_y = min(max(text_y, y_min + (y_max - y_min) * 0.15), y_max * 0.85)\n",
    "                \n",
    "                # Add connecting line from box to annotation\n",
    "                # ax.annotate(\n",
    "                #     stats_text,\n",
    "                #     xy=(i, median_val),  # Start from the median line\n",
    "                #     xytext=(text_x, text_y),  # End at the text position\n",
    "                #     textcoords=\"data\",\n",
    "                #     ha='left',\n",
    "                #     va='center',\n",
    "                #     bbox=dict(boxstyle=\"round,pad=0.5\", fc=\"white\", ec=\"gray\", alpha=0.9),\n",
    "                #     arrowprops=dict(arrowstyle=\"-\", color=\"gray\", connectionstyle=\"arc3,rad=0.3\")\n",
    "                # )\n",
    "            \n",
    "            # Set axis properties\n",
    "            ax.set_xticks(positions)\n",
    "            ax.set_xticklabels(methods)\n",
    "            \n",
    "            # Set titles and labels\n",
    "            title = f'Query Time Distribution - {dataset}'\n",
    "            subtitle = f'Using minimum query times across {run_count} runs'\n",
    "            ax.set_title(f'{title}\\n{subtitle}', pad=20)\n",
    "            ax.set_xlabel('Query Method')\n",
    "            ax.set_ylabel('Execution Time (seconds)')\n",
    "            \n",
    "            # Plot actual data points with jitter for better visibility\n",
    "            for i, method in enumerate(methods):\n",
    "                # Get data for this method\n",
    "                method_data = dataset_df[dataset_df['method'] == method]\n",
    "                \n",
    "                # Get minimum times for each query across runs\n",
    "                min_times_by_query = method_data.groupby(['query #'])['Time (sec)'].min().values\n",
    "                \n",
    "                # Create jitter for better point separation\n",
    "                jitter = np.random.uniform(-width/3, width/3, size=len(min_times_by_query))\n",
    "                \n",
    "                # Plot individual points with semi-transparency\n",
    "                ax.scatter(\n",
    "                    [i + j for j in jitter], \n",
    "                    min_times_by_query,\n",
    "                    s=30,\n",
    "                    alpha=0.5,\n",
    "                    color=GLOBAL_METHOD_COLORS[method],\n",
    "                    edgecolor='black',\n",
    "                    linewidth=0.5,\n",
    "                    zorder=3\n",
    "                )\n",
    "            \n",
    "            # Apply publication styling but without legend\n",
    "            set_publication_style(ax)\n",
    "            \n",
    "            # Adjust y-axis range\n",
    "            y_min, y_max = ax.get_ylim()\n",
    "            margin = (y_max - y_min) * 0.05\n",
    "            ax.set_ylim(y_min - margin, y_max + margin)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save figure\n",
    "            filename_safe = dataset.replace(' ', '_').lower()\n",
    "            plt.savefig(os.path.join(FIGURES_DIR, f\"min_time_distribution_{filename_safe}.pdf\"))\n",
    "            plt.savefig(os.path.join(FIGURES_DIR, f\"min_time_distribution_{filename_safe}.png\"))\n",
    "            plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"No data available for distribution analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "86a8f4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "def extract_patterns(filepath):\n",
    "    # Extract main [start to end] intervals per pattern\n",
    "    with open(filepath, \"r\") as f:\n",
    "        content = f.read()\n",
    "    pattern = re.compile(r\"Match #\\d+: \\[(\\d+) to (\\d+)\\]\")\n",
    "    return set((int(m.group(1)), int(m.group(2))) for m in pattern.finditer(content))\n",
    "\n",
    "def compute_f1(gt_file, pred_file):\n",
    "    gt_patterns = extract_patterns(gt_file)\n",
    "    pred_patterns = extract_patterns(pred_file)\n",
    "    # For F1, construct binary indicator vectors over union of all intervals\n",
    "    all_patterns = sorted(gt_patterns | pred_patterns)\n",
    "    gt_labels = [1 if p in gt_patterns else 0 for p in all_patterns]\n",
    "    pred_labels = [1 if p in pred_patterns else 0 for p in all_patterns]\n",
    "    precision = precision_score(gt_labels, pred_labels)\n",
    "    recall = recall_score(gt_labels, pred_labels)\n",
    "    f1 = f1_score(gt_labels, pred_labels)\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1 Score:  {f1:.4f}\")\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b141a520",
   "metadata": {},
   "source": [
    "## Pattern Detection Accuracy Comparison\n",
    "\n",
    "Compare the accuracy of pattern detection methods against the ground truth using the F1 score metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "37ceb1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pattern detection accuracy comparison...\n",
      "No comparison results generated.\n",
      "No pattern match accuracy results available.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "# Function to find ground truth pattern match files\n",
    "def find_ground_truth_files(db, dataset):\n",
    "    \"\"\"\n",
    "    Find all ground truth pattern match log files\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    base_dir : str\n",
    "        Base directory to search for ground truth files\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    list: List of ground truth file information\n",
    "    \"\"\" \n",
    "    # Get the absolute path to the pattern_matches directory\n",
    "    pattern_dir = os.path.join(\"..\", f\"{outFolder}/pattern_matches/ground_truth/{db}/{dataset}\")\n",
    "    \n",
    "    # Check if directory exists\n",
    "    if not os.path.exists(pattern_dir):\n",
    "        print(f\"Directory '{pattern_dir}' does not exist.\")\n",
    "        return []\n",
    "    \n",
    "    # Find all ground truth log files\n",
    "    gt_pattern = os.path.join(pattern_dir, \"*.log\")\n",
    "    gt_files = glob.glob(gt_pattern)\n",
    "    \n",
    "    if not gt_files:\n",
    "        print(f\"No ground truth files found in '{pattern_dir}'.\")\n",
    "        return []\n",
    "    \n",
    "    # Parse ground truth files information\n",
    "    gt_info = []\n",
    "    method_pattern = re.compile(r\"(.+?)_(\\d+)_(\\d+)_(.+?)\\.log$\")\n",
    "    \n",
    "    for file_path in gt_files:\n",
    "        filename = os.path.basename(file_path)\n",
    "        match = method_pattern.match(filename)\n",
    "        \n",
    "        if match:\n",
    "            start_ts = match.group(1)\n",
    "            end_ts = match.group(2)\n",
    "            measure = match.group(3)\n",
    "            time_unit = match.group(4)\n",
    "\n",
    "            gt_info.append({\n",
    "                'path': file_path,\n",
    "                'database': db,\n",
    "                'dataset': dataset,\n",
    "                'start_ts': start_ts,\n",
    "                'end_ts': end_ts,\n",
    "                'measure': measure,\n",
    "                'time_unit': time_unit,\n",
    "                'filename': filename\n",
    "            })\n",
    "    \n",
    "    print(f\"Found {len(gt_info)} ground truth files.\")\n",
    "    return gt_info\n",
    "\n",
    "# Function to find corresponding method files for a ground truth file\n",
    "def find_method_file(gt_info, method, base_dir=\"pattern_matches\"):\n",
    "    \"\"\"\n",
    "    Find method files that correspond to a ground truth file\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gt_info : dict\n",
    "        Ground truth file information\n",
    "    method : string\n",
    "        List of method information dictionaries\n",
    "    base_dir : str\n",
    "        Base directory to search for method files\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict: Dictionary of method files\n",
    "    \"\"\"\n",
    "    method_files = {}\n",
    "    \n",
    "    method_name = method[\"patternMethod\"]\n",
    "    # Construct a pattern to match files for this method and dataset\n",
    "    method_pattern = f\"{method_name}/{gt_info['database']}/{gt_info['dataset']}/{gt_info['start_ts']}_{gt_info['end_ts']}_{gt_info['measure']}_{gt_info['time_unit']}.log\"\n",
    "    method_path = os.path.join(\"..\", f\"{outFolder}/{base_dir}\", method_pattern)\n",
    "\n",
    "    if os.path.exists(method_path):\n",
    "        return {\n",
    "            'path': method_path,\n",
    "            'method_name': method_name,\n",
    "            'display_name': method[\"name\"]\n",
    "        }\n",
    "    return None;\n",
    "\n",
    "# Compare pattern detection accuracy for all methods against ground truth\n",
    "def compare_pattern_detection(methods, base_dir=\"pattern_matches\"):\n",
    "    \"\"\"\n",
    "    Compare pattern detection accuracy for all methods against ground truth\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    methods : list\n",
    "        List of method information dictionaries\n",
    "    base_dir : str\n",
    "        Base directory containing pattern match files\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame: DataFrame containing accuracy metrics\n",
    "    \"\"\"\n",
    " \n",
    "    results = []\n",
    "    \n",
    "    # For each ground truth file, find corresponding method files and compare\n",
    "    for i, method in enumerate(METHODS):\n",
    "        if(i == 0): continue  # Skip the first method (baseline)\n",
    "        for dataset in all_datasets:\n",
    "            gt_files = find_ground_truth_files(method['database'], dataset)\n",
    "\n",
    "            for gt_file in gt_files:\n",
    "                # Find method files for this ground truth\n",
    "                method_file = find_method_file(gt_file, method, base_dir)\n",
    "                if not method_file:\n",
    "                    print(f\"No method file found for method {method['name']} and dataset {dataset}\")\n",
    "                    continue\n",
    "                # Load ground truth patterns\n",
    "                gt_path = gt_file['path']\n",
    "                try:\n",
    "                    gt_patterns = extract_patterns(gt_path)\n",
    "                    print(f\"Ground truth: {os.path.basename(gt_path)}\")\n",
    "                    print(f\"Found {len(gt_patterns)} ground truth patterns\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading ground truth file: {str(e)}\")\n",
    "                    continue\n",
    "                \n",
    "                # Compare each method against ground truth\n",
    "                pred_path = method_file['path']\n",
    "                display_name = method_file['display_name']\n",
    "                \n",
    "                print(f\"\\nComparing {display_name} vs ground truth:\")\n",
    "                print(f\"Method file: {os.path.basename(pred_path)}\")\n",
    "                \n",
    "                try:\n",
    "                    # Load predicted patterns\n",
    "                    pred_patterns = extract_patterns(pred_path)\n",
    "                    print(f\"Found {len(pred_patterns)} predicted patterns\")\n",
    "                    \n",
    "                    # FIXED CALCULATION: Calculate metrics using set operations\n",
    "                    # True Positives: patterns that are in both ground truth and predictions\n",
    "                    true_positives = len(gt_patterns & pred_patterns)\n",
    "                    \n",
    "                    # False Positives: patterns predicted but not in ground truth\n",
    "                    false_positives = len(pred_patterns - gt_patterns)\n",
    "                    \n",
    "                    # False Negatives: patterns in ground truth but not predicted\n",
    "                    false_negatives = len(gt_patterns - pred_patterns)\n",
    "                    \n",
    "                    # Calculate precision, recall, and F1 using the standard formulas\n",
    "                    if true_positives + false_positives == 0:\n",
    "                        precision = 1.0 if true_positives + false_negatives == 0 else 0.0\n",
    "                    else:\n",
    "                        precision = true_positives / (true_positives + false_positives)\n",
    "                    \n",
    "                    if true_positives + false_negatives == 0:\n",
    "                        recall = 1.0 if true_positives + false_positives == 0 else 0.0\n",
    "                    else:\n",
    "                        recall = true_positives / (true_positives + false_negatives)\n",
    "                    \n",
    "                    if precision + recall == 0:\n",
    "                        f1 = 0.0\n",
    "                    else:\n",
    "                        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "                    \n",
    "                    # Count number of patterns\n",
    "                    num_gt_patterns = len(gt_patterns)\n",
    "                    num_pred_patterns = len(pred_patterns)\n",
    "                    num_correct_patterns = true_positives\n",
    "                    \n",
    "                    results.append({\n",
    "                        'dataset': dataset,\n",
    "                        'method': display_name,\n",
    "                        'method_id': method_file['method_name'],\n",
    "                        'measure': gt_file['measure'],\n",
    "                        'time_unit': gt_file['time_unit'],\n",
    "                        'precision': precision,\n",
    "                        'recall': recall,\n",
    "                        'f1': f1,\n",
    "                        'gt_patterns': num_gt_patterns,\n",
    "                        'pred_patterns': num_pred_patterns,\n",
    "                        'correct_patterns': num_correct_patterns,\n",
    "                        'true_positives': true_positives,\n",
    "                        'false_positives': false_positives,\n",
    "                        'false_negatives': false_negatives\n",
    "                    })\n",
    "                    \n",
    "                    print(f\"True Positives: {true_positives}\")\n",
    "                    print(f\"False Positives: {false_positives}\")\n",
    "                    print(f\"False Negatives: {false_negatives}\")\n",
    "                    print(f\"Precision:  {precision:.4f}\")\n",
    "                    print(f\"Recall:     {recall:.4f}\")\n",
    "                    print(f\"F1 Score:   {f1:.4f}\")\n",
    "                    print(f\"GT Patterns: {num_gt_patterns}, Predicted: {num_pred_patterns}, Correct: {num_correct_patterns}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error comparing {display_name} to ground truth: {str(e)}\")\n",
    "    \n",
    "    # Convert results to DataFrame\n",
    "    if results:\n",
    "        results_df = pd.DataFrame(results)\n",
    "        return results_df\n",
    "    else:\n",
    "        print(\"No comparison results generated.\")\n",
    "        return None\n",
    "\n",
    "# Run the pattern detection accuracy comparison\n",
    "print(\"Starting pattern detection accuracy comparison...\")\n",
    "accuracy_results = compare_pattern_detection(METHODS, \"pattern_matches\")\n",
    "\n",
    "if accuracy_results is not None:\n",
    "    print(\"\\nSummary of pattern detection accuracy:\")\n",
    "    display(accuracy_results)\n",
    "    \n",
    "    # Create visualizations if we have results\n",
    "    if not accuracy_results.empty:\n",
    "        # Plot F1 scores by dataset and method\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Group by dataset and method to get mean F1 scores\n",
    "        summary = accuracy_results.groupby(['dataset', 'method'])['f1'].mean().reset_index()\n",
    "        \n",
    "        # # Create a pivot table for better visualization\n",
    "        # pivot_data = summary.pivot(index='dataset', columns='method', values='f1')\n",
    "        \n",
    "        # # Plot heatmap\n",
    "        # ax = sns.heatmap(pivot_data, annot=True, fmt=\".3f\", cmap=\"YlGnBu\", \n",
    "        #                vmin=0, vmax=1, linewidths=0.5)\n",
    "        # plt.title(\"Pattern Detection F1 Score by Dataset and Method\", pad=20)\n",
    "        # plt.tight_layout()\n",
    "        \n",
    "        # # Save figure\n",
    "        # plt.savefig(os.path.join(FIGURES_DIR, \"pattern_detection_f1_scores.pdf\"))\n",
    "        # plt.savefig(os.path.join(FIGURES_DIR, \"pattern_detection_f1_scores.png\"))\n",
    "        # plt.show()\n",
    "        \n",
    "        # Create bar charts for individual metrics\n",
    "        metrics = ['precision', 'recall', 'f1']\n",
    "        fig, axes = plt.subplots(len(metrics), 1, figsize=(12, 15))\n",
    "        \n",
    "        for i, metric in enumerate(metrics):\n",
    "            # Group by dataset and method\n",
    "            summary = accuracy_results.groupby(['dataset', 'method'])[metric].mean().reset_index()\n",
    "            \n",
    "            # Create the grouped bar chart with consistent colors\n",
    "            method_names = summary['method'].unique()\n",
    "            palette = [GLOBAL_METHOD_COLORS.get(method, '#1f77b4') for method in method_names]\n",
    "            sns.barplot(data=summary, x='dataset', y=metric, hue='method', ax=axes[i], palette=palette)\n",
    "            \n",
    "            # Set titles and labels\n",
    "            axes[i].set_title(f\"Pattern Detection {metric.capitalize()} by Dataset and Method\", pad=10)\n",
    "            axes[i].set_xlabel('Dataset')\n",
    "            axes[i].set_ylabel(metric.capitalize())\n",
    "            \n",
    "            # Apply publication styling\n",
    "            set_publication_style(axes[i], legend_title='Method')\n",
    "            \n",
    "            # Adjust y-axis limits\n",
    "            axes[i].set_ylim(0, 1.05)\n",
    "            \n",
    "            # Add text labels above bars\n",
    "            for p in axes[i].patches:\n",
    "                axes[i].annotate(f\"{p.get_height():.3f}\", \n",
    "                              (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                              ha = 'center', va = 'bottom')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save figure\n",
    "        plt.savefig(os.path.join(FIGURES_DIR, \"pattern_detection_metrics.pdf\"))\n",
    "        plt.savefig(os.path.join(FIGURES_DIR, \"pattern_detection_metrics.png\"))\n",
    "        plt.show()\n",
    "        \n",
    "        # Create a summary table showing overall metrics by method\n",
    "        method_summary = accuracy_results.groupby('method').agg({\n",
    "            'precision': ['mean', 'std'],\n",
    "            'recall': ['mean', 'std'],\n",
    "            'f1': ['mean', 'std'],\n",
    "            'dataset': 'nunique'\n",
    "        }).reset_index()\n",
    "        \n",
    "        # Flatten the multi-level column names\n",
    "        method_summary.columns = ['_'.join(col).strip('_') for col in method_summary.columns]\n",
    "        \n",
    "        # Rename columns for clarity\n",
    "        method_summary = method_summary.rename(columns={\n",
    "            'method_': 'method',\n",
    "            'precision_mean': 'avg_precision',\n",
    "            'precision_std': 'std_precision',\n",
    "            'recall_mean': 'avg_recall',\n",
    "            'recall_std': 'std_recall',\n",
    "            'f1_mean': 'avg_f1',\n",
    "            'f1_std': 'std_f1',\n",
    "            'dataset_nunique': 'num_datasets'\n",
    "        })\n",
    "else:\n",
    "    print(\"No pattern match accuracy results available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1492b519",
   "metadata": {},
   "source": [
    "## Visual Similarity Comparison between Query Results\n",
    "\n",
    "Compare the visual similarity of query results between different methods and the ground truth (M4-NoC) using two complementary metrics:\n",
    "\n",
    "1. **Structural Similarity Index Measure (SSIM)**: Measures perceptual similarity between images, considering luminance, contrast, and structure. Values range from 0 to 1, where 1 means identical images.\n",
    "\n",
    "2. **Pixel Difference Percentage**: Measures the percentage of pixels that differ between two images. Values range from 0% to 100%, where 0% means identical images and 100% means completely different images.\n",
    "\n",
    "**Note:** The analysis is separated into:\n",
    "1. **Calculation Cell**: Computes both SSIM scores and pixel difference percentages, caching results to `./analysis_cache/similarity_data.pkl`\n",
    "2. **Plotting Cell**: Generates visualizations using cached data for both metrics\n",
    "3. **Cache Management Cell**: Utilities to clear cache and force recalculation\n",
    "\n",
    "This separation allows you to:\n",
    "- Run expensive calculations once and reuse results\n",
    "- Quickly regenerate plots without recalculating similarity metrics\n",
    "- Force recalculation when needed by setting `force_recalculate = True` or clearing cache\n",
    "\n",
    "The two metrics provide complementary information:\n",
    "- **SSIM** is better for understanding perceptual similarity and structural differences\n",
    "- **Pixel Difference Percentage** provides a straightforward measure of how many pixels actually differ between images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8a4f529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import re\n",
    "import tempfile\n",
    "import shutil\n",
    "from PIL import Image\n",
    "\n",
    "# Import cairo_plot module for plotting time series\n",
    "sys.path.append(\"..\")\n",
    "from cairo_plot import plot, compute_ssim, compute_pixel_difference_percentage\n",
    "\n",
    "# Create cache directory for saving computed data\n",
    "CACHE_DIR = \"./analysis_cache\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dd8314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cache directory for saving computed data\n",
    "CACHE_DIR = \"./analysis_cache\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "# Function to convert timestamp to epoch milliseconds in UTC\n",
    "def convert_to_epoch_millis_utc(timestamp_str):\n",
    "    \"\"\"\n",
    "    Convert timestamp string to epoch milliseconds in UTC.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    timestamp_str : str or int or float\n",
    "        Timestamp in various formats (ISO string, epoch seconds, epoch milliseconds)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    int: Epoch milliseconds in UTC\n",
    "    \"\"\"\n",
    "    from datetime import datetime, timezone\n",
    "    import dateutil.parser\n",
    "    \n",
    "    if pd.isna(timestamp_str) or timestamp_str is None:\n",
    "        return None\n",
    "    \n",
    "    # If it's already a number, assume it's epoch time\n",
    "    if isinstance(timestamp_str, (int, float)):\n",
    "        # Check if it's in seconds (typical range) or milliseconds\n",
    "        if timestamp_str < 1e12:  # Likely in seconds\n",
    "            return int(timestamp_str * 1000)\n",
    "        else:  # Likely already in milliseconds\n",
    "            return int(timestamp_str)\n",
    "    \n",
    "    # Convert string to number if possible\n",
    "    try:\n",
    "        num_val = float(timestamp_str)\n",
    "        if num_val < 1e12:  # Likely in seconds\n",
    "            return int(num_val * 1000)\n",
    "        else:  # Likely already in milliseconds\n",
    "            return int(num_val)\n",
    "    except (ValueError, TypeError):\n",
    "        pass\n",
    "    \n",
    "    # Try to parse as ISO datetime string\n",
    "    try:\n",
    "        dt = dateutil.parser.parse(timestamp_str)\n",
    "        # Convert to UTC if timezone aware, otherwise assume UTC\n",
    "        if dt.tzinfo is None:\n",
    "            dt = dt.replace(tzinfo=timezone.utc)\n",
    "        else:\n",
    "            dt = dt.astimezone(timezone.utc)\n",
    "        return int(dt.timestamp() * 1000)\n",
    "    except:\n",
    "        print(f\"Warning: Could not parse timestamp: {timestamp_str}\")\n",
    "        return None\n",
    "\n",
    "# Function to find all query CSV files for a specific dataset and method\n",
    "def find_query_csvs(base_dir, method, database, dataset):\n",
    "    \"\"\"\n",
    "    Find all query result CSV files for a specific dataset and method.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    base_dir : str\n",
    "        Base directory path\n",
    "    method : str\n",
    "        Method name (e.g., 'm4', 'm4Inf')\n",
    "    database : str\n",
    "        Database name (e.g., 'influx')\n",
    "    dataset : str\n",
    "        Dataset name\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    list: List of dictionaries containing query information\n",
    "    \"\"\"\n",
    "    query_dir = os.path.join(base_dir, method, database, dataset)\n",
    "    query_files = []\n",
    "    \n",
    "    # Check if the directory exists\n",
    "    if not os.path.exists(query_dir):\n",
    "        print(f\"Directory not found: {query_dir}\")\n",
    "        return query_files\n",
    "    \n",
    "    # Find all run directories\n",
    "    run_dirs = glob.glob(os.path.join(query_dir, \"run_*\"))\n",
    "    \n",
    "    for run_dir in run_dirs:\n",
    "        run_name = os.path.basename(run_dir)\n",
    "        result_csv = os.path.join(run_dir, \"results.csv\")\n",
    "        \n",
    "        if not os.path.exists(result_csv):\n",
    "            print(f\"Results CSV not found: {result_csv}\")\n",
    "            continue\n",
    "            \n",
    "        df_result = pd.read_csv(result_csv)\n",
    "        print(f\"Loaded results CSV with {len(df_result)} rows\")\n",
    "        \n",
    "        # Find all query directories within this run\n",
    "        query_dirs = glob.glob(os.path.join(run_dir, \"query_*\"))\n",
    "        \n",
    "        for query_dir in query_dirs:\n",
    "            query_name = os.path.basename(query_dir)\n",
    "            query_id = int(query_name.split(\"_\")[1])\n",
    "            \n",
    "            # Get query time range from results CSV\n",
    "            query_row = df_result[df_result['query #'] == query_id]\n",
    "            if query_row.empty:\n",
    "                print(f\"Query {query_id} not found in results CSV\")\n",
    "                continue\n",
    "                \n",
    "            query_start_raw = query_row['from'].values[0]\n",
    "            query_end_raw = query_row['to'].values[0]\n",
    "            \n",
    "            # Convert to epoch milliseconds in UTC\n",
    "            query_start_millis = convert_to_epoch_millis_utc(query_start_raw)\n",
    "            query_end_millis = convert_to_epoch_millis_utc(query_end_raw)\n",
    "            \n",
    "            if query_start_millis is None or query_end_millis is None:\n",
    "                print(f\"Could not parse timestamps for query {query_id}: {query_start_raw} -> {query_end_raw}\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"Query {query_id}: {query_start_raw} -> {query_start_millis}, {query_end_raw} -> {query_end_millis}\")\n",
    "            \n",
    "            # Find all CSV files in this query directory\n",
    "            csv_files = glob.glob(os.path.join(query_dir, \"*.csv\"))\n",
    "            \n",
    "            for csv_file in csv_files:\n",
    "                measure_id = os.path.basename(csv_file).split(\".\")[0]\n",
    "                \n",
    "                query_files.append({\n",
    "                    'path': csv_file,\n",
    "                    'run': run_name,\n",
    "                    'query': query_name,\n",
    "                    'query_id': query_id,\n",
    "                    'measure': measure_id,\n",
    "                    'dataset': dataset,\n",
    "                    'method': method,\n",
    "                    'start_time': query_start_millis,\n",
    "                    'end_time': query_end_millis,\n",
    "                    'duration': query_end_millis - query_start_millis,\n",
    "                    'from_raw': query_start_raw,\n",
    "                    'to_raw': query_end_raw\n",
    "                })\n",
    "    \n",
    "    return query_files\n",
    "\n",
    "# Function to generate plot images and compute similarity metrics\n",
    "def compute_query_similarity_metrics(datasets, methods, temp_dir):\n",
    "    \"\"\"\n",
    "    Generate plots and compute SSIM scores and pixel difference percentages between methods and ground truth.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    datasets : list\n",
    "        List of dataset names\n",
    "    methods : list\n",
    "        List of method configurations\n",
    "    temp_dir : str\n",
    "        Temporary directory for storing images\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame: DataFrame containing SSIM scores and pixel difference percentages\n",
    "    \"\"\"\n",
    "    similarity_results = []\n",
    "    \n",
    "    # Get the first method as ground truth\n",
    "    gt_method = methods[0]\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        print(f\"Processing dataset: {dataset}\")\n",
    "        \n",
    "        # Find all query CSVs for ground truth method\n",
    "        gt_csvs = find_query_csvs(\n",
    "            gt_method['path'], \n",
    "            gt_method['method'], \n",
    "            gt_method['database'], \n",
    "            dataset\n",
    "        )\n",
    "\n",
    "        if not gt_csvs:\n",
    "            print(f\"No ground truth CSVs found for dataset {dataset}\")\n",
    "            continue\n",
    "        \n",
    "        # Group ground truth files by query and measure\n",
    "        gt_by_query = {}\n",
    "        for gt_csv in gt_csvs:\n",
    "            key = (gt_csv['query_id'], gt_csv['measure'])\n",
    "            gt_by_query[key] = gt_csv\n",
    "        \n",
    "        # For each comparison method\n",
    "        for method_idx, method in enumerate(methods[1:], 1):  # Skip the ground truth method\n",
    "            method_name = method['name']\n",
    "            print(f\"  Comparing {method_name} with ground truth\")\n",
    "            \n",
    "            # Find all query CSVs for this method\n",
    "            method_csvs = find_query_csvs(\n",
    "                method['path'], \n",
    "                method['method'], \n",
    "                method['database'], \n",
    "                dataset\n",
    "            )\n",
    "            \n",
    "            if not method_csvs:\n",
    "                print(f\"  No CSVs found for method {method_name}\")\n",
    "                continue\n",
    "            \n",
    "            # Group method files by query and measure\n",
    "            method_by_query = {}\n",
    "            for method_csv in method_csvs:\n",
    "                key = (method_csv['query_id'], method_csv['measure'])\n",
    "                method_by_query[key] = method_csv\n",
    "            \n",
    "            # Find common queries between ground truth and method\n",
    "            common_keys = set(gt_by_query.keys()) & set(method_by_query.keys())\n",
    "            print(f\"  Found {len(common_keys)} common queries to compare\")\n",
    "            \n",
    "            # For each common query, generate plots and compute similarity metrics\n",
    "            for query_key in common_keys:\n",
    "                query_id, measure = query_key\n",
    "                \n",
    "                gt_csv = gt_by_query[query_key]\n",
    "                method_csv = method_by_query[query_key]\n",
    "                \n",
    "                try:\n",
    "                    # Load CSV data\n",
    "                    gt_df = pd.read_csv(gt_csv['path'])\n",
    "                    method_df = pd.read_csv(method_csv['path'])\n",
    "                    \n",
    "                    # Use the converted epoch milliseconds for plot range\n",
    "                    query_start = gt_csv['start_time']\n",
    "                    query_end = gt_csv['end_time']\n",
    "                                        \n",
    "                    # Create temporary image files\n",
    "                    gt_img_path = os.path.join(temp_dir, f\"{dataset}_{query_id}_{measure}_gt.png\")\n",
    "                    method_img_path = os.path.join(temp_dir, f\"{dataset}_{query_id}_{measure}_method{method_idx}.png\")\n",
    "                \n",
    "                    # Generate plots using cairo_plot with epoch milliseconds\n",
    "                    plot(gt_df, measure, gt_img_path.replace('.png', ''), 1000, 600, query_start, query_end)\n",
    "                    plot(method_df, measure, method_img_path.replace('.png', ''), 1000, 600, query_start, query_end)\n",
    "                    \n",
    "                    # Compute SSIM between the two images\n",
    "                    ssim_score = compute_ssim(gt_img_path, method_img_path)\n",
    "                    \n",
    "                    # Compute pixel difference percentage between the two images\n",
    "                    pixel_diff_percentage = compute_pixel_difference_percentage(gt_img_path, method_img_path)\n",
    "                    \n",
    "                    # Save results with time information\n",
    "                    similarity_results.append({\n",
    "                        'dataset': dataset,\n",
    "                        'query_id': query_id,\n",
    "                        'measure': measure,\n",
    "                        'method': method_name,\n",
    "                        'ssim': ssim_score,\n",
    "                        'pixel_diff_percentage': pixel_diff_percentage,\n",
    "                        'start_time': query_start,\n",
    "                        'end_time': query_end,\n",
    "                        'duration': gt_csv['duration'],\n",
    "                        'from_raw': gt_csv['from_raw'],\n",
    "                        'to_raw': gt_csv['to_raw']\n",
    "                    })\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"  Error processing {dataset} query {query_id} measure {measure}: {str(e)}\")\n",
    "    \n",
    "    # Convert results to DataFrame\n",
    "    if similarity_results:\n",
    "        similarity_df = pd.DataFrame(similarity_results)\n",
    "        return similarity_df\n",
    "    else:\n",
    "        print(\"No similarity results generated.\")\n",
    "        return None\n",
    "\n",
    "# Check if similarity data is already cached\n",
    "similarity_cache_file = os.path.join(CACHE_DIR, \"similarity_data.pkl\")\n",
    "force_recalculate = False  # Set this to True to force recalculation\n",
    "\n",
    "if os.path.exists(similarity_cache_file) and not force_recalculate:\n",
    "    print(\"Loading cached similarity data...\")\n",
    "    with open(similarity_cache_file, 'rb') as f:\n",
    "        cached_data = pickle.load(f)\n",
    "        similarity_df = cached_data.get('similarity_df')\n",
    "        similarity_summary = cached_data.get('similarity_summary')\n",
    "    print(f\"Loaded cached similarity data with {len(similarity_df)} records\")\n",
    "else:\n",
    "    print(\"Computing similarity metrics...\")\n",
    "    \n",
    "    # Create temporary directory for images\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "    \n",
    "    try:\n",
    "        print(f\"Created temporary directory: {temp_dir}\")\n",
    "        \n",
    "        # Compute similarity metrics\n",
    "        print(\"Computing SSIM scores and pixel difference percentages between query results...\")\n",
    "        similarity_df = compute_query_similarity_metrics(all_datasets, METHODS, temp_dir)\n",
    "        \n",
    "        if similarity_df is not None and not similarity_df.empty:\n",
    "            # Display summary of similarity metrics\n",
    "            print(\"\\nSummary of similarity metrics by dataset and method:\")\n",
    "            similarity_summary = similarity_df.groupby(['dataset', 'method'])[['ssim', 'pixel_diff_percentage']].agg(['mean', 'median', 'std', 'min', 'max', 'count']).reset_index()\n",
    "            display(similarity_summary)\n",
    "            \n",
    "            # Display time range information if available\n",
    "            if 'start_time' in similarity_df.columns:\n",
    "                print(\"\\nQuery time range summary (epoch milliseconds UTC):\")\n",
    "                time_summary = similarity_df.groupby(['dataset', 'method']).agg({\n",
    "                    'start_time': ['min', 'max'],\n",
    "                    'end_time': ['min', 'max'],\n",
    "                    'duration': ['mean', 'median', 'std']\n",
    "                }).reset_index()\n",
    "                display(time_summary)\n",
    "                \n",
    "                # Also show some example raw timestamps for verification\n",
    "                print(\"\\nExample timestamp conversions:\")\n",
    "                sample_data = similarity_df[['dataset', 'method', 'query_id', 'from_raw', 'to_raw', 'start_time', 'end_time', 'duration']].head(3)\n",
    "                display(sample_data)\n",
    "            \n",
    "            # Cache the computed data\n",
    "            cache_data = {\n",
    "                'similarity_df': similarity_df,\n",
    "                'similarity_summary': similarity_summary\n",
    "            }\n",
    "            with open(similarity_cache_file, 'wb') as f:\n",
    "                pickle.dump(cache_data, f)\n",
    "            print(f\"Cached similarity data to {similarity_cache_file}\")\n",
    "        else:\n",
    "            print(\"No similarity results available.\")\n",
    "            similarity_df = None\n",
    "            similarity_summary = None\n",
    "\n",
    "    finally:\n",
    "        # Clean up the temporary directory\n",
    "        if 'temp_dir' in locals():\n",
    "            shutil.rmtree(temp_dir)\n",
    "            print(f\"Removed temporary directory: {temp_dir}\")\n",
    "\n",
    "# For backward compatibility, create ssim_df and ssim_summary variables\n",
    "if 'similarity_df' in locals() and similarity_df is not None:\n",
    "    ssim_df = similarity_df.copy()\n",
    "    ssim_summary = similarity_df.groupby(['dataset', 'method'])['ssim'].agg(['mean', 'median', 'std', 'min', 'max', 'count']).reset_index()\n",
    "else:\n",
    "    ssim_df = None\n",
    "    ssim_summary = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4525fe81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity Metrics Plotting - Run this cell to generate SSIM and pixel difference visualizations\n",
    "# This cell uses cached data from the previous calculation cell\n",
    "\n",
    "if 'similarity_df' in globals() and similarity_df is not None and not similarity_df.empty:\n",
    "    print(\"Generating similarity metrics visualizations...\")\n",
    "    \n",
    "    # Get unique datasets and methods\n",
    "    datasets = similarity_df['dataset'].unique()\n",
    "    methods = similarity_df['method'].unique()\n",
    "    \n",
    "    # Create a mapping of methods to colors using the global method color mapping\n",
    "    method_color_map = {}\n",
    "    for method in methods:\n",
    "        method_color_map[method] = GLOBAL_METHOD_COLORS.get(method, '#1f77b4')  # Fallback to blue if not found\n",
    "    \n",
    "    print(f\"Method color mapping: {method_color_map}\")\n",
    "    \n",
    "    # Create box plots of SSIM scores by dataset\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 16))\n",
    "    \n",
    "    # SSIM Box Plot\n",
    "    n_datasets = len(datasets)\n",
    "    n_methods = len(methods)\n",
    "    width = 0.6 / n_methods\n",
    "    \n",
    "    for ds_idx, dataset in enumerate(datasets):\n",
    "        for method_idx, method in enumerate(methods):\n",
    "            # Get SSIM data for this dataset-method combination\n",
    "            ds_method_data = similarity_df[\n",
    "                (similarity_df['dataset'] == dataset) & \n",
    "                (similarity_df['method'] == method)\n",
    "            ]['ssim']\n",
    "            \n",
    "            if len(ds_method_data) == 0:\n",
    "                continue\n",
    "                \n",
    "            # Calculate statistics\n",
    "            mean_val = ds_method_data.mean()\n",
    "            median_val = ds_method_data.median()\n",
    "            min_val = ds_method_data.min()\n",
    "            max_val = ds_method_data.max()\n",
    "            std_val = ds_method_data.std()\n",
    "            \n",
    "            # Position for this box\n",
    "            x_pos = ds_idx + (method_idx - n_methods/2 + 0.5) * width\n",
    "            \n",
    "            # Draw box\n",
    "            box_bottom = max(0, median_val - std_val/2)\n",
    "            box_height = std_val\n",
    "            box = plt.Rectangle(\n",
    "                (x_pos - width/2, box_bottom),\n",
    "                width, box_height,\n",
    "                alpha=0.7,\n",
    "                facecolor=method_color_map[method],\n",
    "                edgecolor='black',\n",
    "                linewidth=1.5\n",
    "            )\n",
    "            ax1.add_patch(box)\n",
    "            \n",
    "            # Draw median line\n",
    "            ax1.plot([x_pos - width/2, x_pos + width/2], [median_val, median_val], \n",
    "                    color='white', linewidth=2.5, solid_capstyle='round')\n",
    "            ax1.plot([x_pos - width/2, x_pos + width/2], [median_val, median_val], \n",
    "                    color='black', linewidth=1.5, solid_capstyle='round')\n",
    "            \n",
    "            # Draw whiskers\n",
    "            ax1.plot([x_pos, x_pos], [max(0, min_val), box_bottom], \n",
    "                    color='black', linewidth=1.5, linestyle='-')\n",
    "            ax1.plot([x_pos, x_pos], [box_bottom + box_height, max_val], \n",
    "                    color='black', linewidth=1.5, linestyle='-')\n",
    "            \n",
    "            # Draw caps on whiskers\n",
    "            whisker_width = width / 4\n",
    "            ax1.plot([x_pos - whisker_width, x_pos + whisker_width], [max(0, min_val), max(0, min_val)], \n",
    "                    color='black', linewidth=1.5)\n",
    "            ax1.plot([x_pos - whisker_width, x_pos + whisker_width], [max_val, max_val], \n",
    "                    color='black', linewidth=1.5)\n",
    "            \n",
    "            # Draw mean point\n",
    "            ax1.plot(x_pos, mean_val, 'o', color='white', markersize=8)\n",
    "            ax1.plot(x_pos, mean_val, 'o', color='black', markersize=6)\n",
    "           \n",
    "            # Plot individual points with jitter\n",
    "            jitter = np.random.uniform(-width/3, width/3, size=len(ds_method_data))\n",
    "            ax1.scatter(\n",
    "                [x_pos + j for j in jitter], \n",
    "                ds_method_data.values,\n",
    "                s=30,\n",
    "                alpha=0.5,\n",
    "                color=method_color_map[method],\n",
    "                edgecolor='black',\n",
    "                linewidth=0.5,\n",
    "                zorder=3\n",
    "            )\n",
    "    \n",
    "    # Set SSIM axis properties\n",
    "    ax1.set_xticks(range(len(datasets)))\n",
    "    ax1.set_xticklabels(datasets, rotation=45, ha='right')\n",
    "    ax1.set_ylim(max(0, similarity_df['ssim'].min() - 0.05), min(1.0, similarity_df['ssim'].max() + 0.05))\n",
    "    \n",
    "    # Create legend for SSIM plot\n",
    "    legend_elements = [plt.Rectangle((0,0),1,1, facecolor=method_color_map[method], \n",
    "                                   edgecolor='black', alpha=0.7, label=method) \n",
    "                      for method in methods if method in method_color_map]\n",
    "    ax1.legend(handles=legend_elements, loc='upper right')\n",
    "    \n",
    "    # Apply publication styling to SSIM plot\n",
    "    set_publication_style(ax1, \n",
    "                         title='SSIM Scores vs Ground Truth (M4-NoC)',\n",
    "                         xlabel='Dataset', \n",
    "                         ylabel='SSIM Score',\n",
    "                        )\n",
    "    \n",
    "    # Pixel Difference Percentage Box Plot\n",
    "    for ds_idx, dataset in enumerate(datasets):\n",
    "        for method_idx, method in enumerate(methods):\n",
    "            # Get pixel difference data for this dataset-method combination\n",
    "            ds_method_data = similarity_df[\n",
    "                (similarity_df['dataset'] == dataset) & \n",
    "                (similarity_df['method'] == method)\n",
    "            ]['pixel_diff_percentage']\n",
    "            \n",
    "            if len(ds_method_data) == 0:\n",
    "                continue\n",
    "                \n",
    "            # Calculate statistics\n",
    "            mean_val = ds_method_data.mean()\n",
    "            median_val = ds_method_data.median()\n",
    "            min_val = ds_method_data.min()\n",
    "            max_val = ds_method_data.max()\n",
    "            std_val = ds_method_data.std()\n",
    "            \n",
    "            # Position for this box\n",
    "            x_pos = ds_idx + (method_idx - n_methods/2 + 0.5) * width\n",
    "            \n",
    "            # Draw box\n",
    "            box_bottom = max(0, median_val - std_val/2)\n",
    "            box_height = std_val\n",
    "            box = plt.Rectangle(\n",
    "                (x_pos - width/2, box_bottom),\n",
    "                width, box_height,\n",
    "                alpha=0.7,\n",
    "                facecolor=method_color_map[method],\n",
    "                edgecolor='black',\n",
    "                linewidth=1.5\n",
    "            )\n",
    "            ax2.add_patch(box)\n",
    "            \n",
    "            # Draw median line\n",
    "            ax2.plot([x_pos - width/2, x_pos + width/2], [median_val, median_val], \n",
    "                    color='white', linewidth=2.5, solid_capstyle='round')\n",
    "            ax2.plot([x_pos - width/2, x_pos + width/2], [median_val, median_val], \n",
    "                    color='black', linewidth=1.5, solid_capstyle='round')\n",
    "            \n",
    "            # Draw whiskers\n",
    "            ax2.plot([x_pos, x_pos], [max(0, min_val), box_bottom], \n",
    "                    color='black', linewidth=1.2, linestyle='-')\n",
    "            ax2.plot([x_pos, x_pos], [box_bottom + box_height, max_val], \n",
    "                    color='black', linewidth=1.2, linestyle='-')\n",
    "            \n",
    "            # Draw caps on whiskers\n",
    "            whisker_width = width / 4\n",
    "            ax2.plot([x_pos - whisker_width, x_pos + whisker_width], [max(0, min_val), max(0, min_val)], \n",
    "                    color='black', linewidth=1.2)\n",
    "            ax2.plot([x_pos - whisker_width, x_pos + whisker_width], [max_val, max_val], \n",
    "                    color='black', linewidth=1.2)\n",
    "            \n",
    "            # Draw mean point\n",
    "            ax2.plot(x_pos, mean_val, 'o', color='white', markersize=6)\n",
    "            ax2.plot(x_pos, mean_val, 'o', color='black', markersize=4)\n",
    "           \n",
    "            # Plot individual points with jitter\n",
    "            jitter = np.random.uniform(-width/3, width/3, size=len(ds_method_data))\n",
    "            ax2.scatter(\n",
    "                [x_pos + j for j in jitter], \n",
    "                ds_method_data.values,\n",
    "                s=20,\n",
    "                alpha=0.5,\n",
    "                color=method_color_map[method],\n",
    "                edgecolor='black',\n",
    "                linewidth=0.3,\n",
    "                zorder=3\n",
    "            )\n",
    "    \n",
    "    # Set pixel difference axis properties and apply publication style\n",
    "    ax2.set_xticks(range(len(datasets)))\n",
    "    ax2.set_xticklabels(datasets, rotation=45, ha='right')\n",
    "    \n",
    "    # Create legend pixel differenceboth lots\n",
    "    legend_elements = [plt.Rectangle((0,0),1,1, facecolor=method_color_map[method], \n",
    "                                   edgecolor='black', alpha=0.7, label=method) \n",
    "                      for method in methods if method in method_color_map]\n",
    "    ax2.legend(handles=legend_elements, loc='upper right')\n",
    "    \n",
    "    # Apply publication styling to pixel difference plot\n",
    "    set_publication_style(ax2, \n",
    "                         title='Pixel Difference % vs Ground Truth (M4-NoC)',\n",
    "                         xlabel='Dataset', \n",
    "                         ylabel='Pixel Difference (%)',\n",
    "                         )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(FIGURES_DIR, \"similarity_metrics_boxplots_by_dataset.pdf\"))\n",
    "    plt.savefig(os.path.join(FIGURES_DIR, \"similarity_metrics_boxplots_by_dataset.png\"))\n",
    "    plt.show()\n",
    "    \n",
    "    # Create heatmaps for both metrics\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    \n",
    "    # SSIM Heatmap - higher values are better (use YlGnBu: yellow-green-blue)\n",
    "    heatmap_data_ssim = similarity_df.groupby(['dataset', 'method'])['ssim'].mean().reset_index()\n",
    "    pivot_data_ssim = heatmap_data_ssim.pivot(index='dataset', columns='method', values='ssim')\n",
    "    \n",
    "    sns.heatmap(pivot_data_ssim, annot=True, fmt='.3f', cmap='YlGnBu', vmin=0, vmax=1, \n",
    "                linewidths=0.5, ax=ax1, cbar_kws={'shrink': 0.8})\n",
    "    \n",
    "    # Apply publication styling to SSIM heatmap\n",
    "    set_publication_style(ax1, \n",
    "                         title='Average SSIM Scores by Dataset and Method\\n(Higher is Better)',\n",
    "                         xlabel='Method', \n",
    "                         ylabel='Dataset')\n",
    "    \n",
    "    # Pixel Difference Percentage Heatmap - lower values are better (use YlGnBu_r: reverse YlGnBu)\n",
    "    heatmap_data_pixel = similarity_df.groupby(['dataset', 'method'])['pixel_diff_percentage'].mean().reset_index()\n",
    "    pivot_data_pixel = heatmap_data_pixel.pivot(index='dataset', columns='method', values='pixel_diff_percentage')\n",
    "    \n",
    "    # Use reversed YlGnBu colormap so that lower values (better) are darker blue/green\n",
    "    sns.heatmap(pivot_data_pixel, annot=True, fmt='.1f', cmap='YlGnBu_r', \n",
    "                linewidths=0.5, ax=ax2, cbar_kws={'shrink': 0.8})\n",
    "    \n",
    "    # Apply publication styling to pixel difference heatmap\n",
    "    set_publication_style(ax2, \n",
    "                         title='Average Pixel Difference % by Dataset and Method\\n(Lower is Better)',\n",
    "                         xlabel='Method', \n",
    "                         ylabel='Dataset')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(FIGURES_DIR, \"similarity_metrics_heatmaps.pdf\"))\n",
    "    plt.savefig(os.path.join(FIGURES_DIR, \"similarity_metrics_heatmaps.png\"))\n",
    "    plt.show()\n",
    "    \n",
    "    # Display summary statistics\n",
    "    print(\"\\n=== SIMILARITY METRICS SUMMARY ===\")\n",
    "    print(\"\\nSSIM Scores Summary:\")\n",
    "    print(similarity_df.groupby(['dataset', 'method'])['ssim'].agg(['mean', 'median', 'std', 'min', 'max', 'count']).round(4))\n",
    "    \n",
    "    print(\"\\nPixel Difference Percentage Summary:\")\n",
    "    print(similarity_df.groupby(['dataset', 'method'])['pixel_diff_percentage'].agg(['mean', 'median', 'std', 'min', 'max', 'count']).round(2))\n",
    "    \n",
    "    print(\"\\nSimilarity metrics visualizations completed!\")\n",
    "    \n",
    "elif 'ssim_df' in globals() and ssim_df is not None and not ssim_df.empty:\n",
    "    # Fallback to original SSIM-only visualization for backward compatibility\n",
    "    print(\"Generating SSIM visualizations (legacy mode)...\")\n",
    "    \n",
    "    # Create box plots of SSIM scores by dataset  \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))  # Smaller figure for double-column\n",
    "    \n",
    "    # Use seaborn for cleaner boxplots with consistent colors\n",
    "    method_names = ssim_df['method'].unique()\n",
    "    palette = [GLOBAL_METHOD_COLORS.get(method, '#1f77b4') for method in method_names]\n",
    "    sns.boxplot(data=ssim_df, x='dataset', y='ssim', hue='method', ax=ax, palette=palette)\n",
    "    \n",
    "    # Apply publication styling\n",
    "    set_publication_style(ax, \n",
    "                         title='SSIM Scores vs Ground Truth (M4-NoC)', \n",
    "                         xlabel='Dataset', \n",
    "                         ylabel='SSIM Score',\n",
    "                         legend_title='Method')\n",
    "    \n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(FIGURES_DIR, \"ssim_comparison_by_dataset.pdf\"))\n",
    "    plt.savefig(os.path.join(FIGURES_DIR, \"ssim_comparison_by_dataset.png\"))\n",
    "    plt.show()\n",
    "    \n",
    "    # Create heatmap\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))  # Smaller figure for double-column\n",
    "    \n",
    "    # Create pivot table for heatmap\n",
    "    ssim_pivot = ssim_df.groupby(['dataset', 'method'])['ssim'].mean().reset_index()\n",
    "    ssim_pivot = ssim_pivot.pivot(index='dataset', columns='method', values='ssim')\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(ssim_pivot, annot=True, fmt='.3f', cmap='YlGnBu', vmin=0, vmax=1, \n",
    "                linewidths=0.5, ax=ax, cbar_kws={'shrink': 0.8})\n",
    "    \n",
    "    # Apply publication styling\n",
    "    set_publication_style(ax, \n",
    "                         title='Average SSIM Scores by Dataset and Method',\n",
    "                         xlabel='Method', \n",
    "                         ylabel='Dataset')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(FIGURES_DIR, \"ssim_heatmap_by_dataset.pdf\"))\n",
    "    plt.savefig(os.path.join(FIGURES_DIR, \"ssim_heatmap_by_dataset.png\"))\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"SSIM visualizations completed!\")\n",
    "    \n",
    "else:\n",
    "    print(\"No similarity data available for plotting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f0180a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache Management Utilities\n",
    "# Use this cell to manage cached analysis data\n",
    "\n",
    "def clear_analysis_cache():\n",
    "    \"\"\"Clear all cached analysis data\"\"\"\n",
    "    if os.path.exists(CACHE_DIR):\n",
    "        shutil.rmtree(CACHE_DIR)\n",
    "        print(f\"Cleared analysis cache directory: {CACHE_DIR}\")\n",
    "    else:\n",
    "        print(\"Cache directory does not exist\")\n",
    "\n",
    "def list_cache_contents():\n",
    "    \"\"\"List contents of the cache directory\"\"\"\n",
    "    if os.path.exists(CACHE_DIR):\n",
    "        cache_files = os.listdir(CACHE_DIR)\n",
    "        if cache_files:\n",
    "            print(f\"Cache directory contents ({CACHE_DIR}):\")\n",
    "            for file in cache_files:\n",
    "                file_path = os.path.join(CACHE_DIR, file)\n",
    "                size = os.path.getsize(file_path)\n",
    "                print(f\"  - {file} ({size:,} bytes)\")\n",
    "        else:\n",
    "            print(\"Cache directory is empty\")\n",
    "    else:\n",
    "        print(\"Cache directory does not exist\")\n",
    "\n",
    "def get_cache_info():\n",
    "    \"\"\"Get information about cached data\"\"\"\n",
    "    similarity_cache_file = os.path.join(CACHE_DIR, \"similarity_data.pkl\")\n",
    "    ssim_cache_file = os.path.join(CACHE_DIR, \"ssim_data.pkl\")  # Legacy file\n",
    "    \n",
    "    # Check new similarity data cache\n",
    "    if os.path.exists(similarity_cache_file):\n",
    "        size = os.path.getsize(similarity_cache_file)\n",
    "        mod_time = os.path.getmtime(similarity_cache_file)\n",
    "        mod_time_str = pd.to_datetime(mod_time, unit='s').strftime('%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        print(f\"Similarity metrics cache file exists:\")\n",
    "        print(f\"  - Size: {size:,} bytes\")\n",
    "        print(f\"  - Modified: {mod_time_str}\")\n",
    "        \n",
    "        # Try to load and show basic info\n",
    "        try:\n",
    "            with open(similarity_cache_file, 'rb') as f:\n",
    "                cached_data = pickle.load(f)\n",
    "                similarity_df = cached_data.get('similarity_df')\n",
    "                if similarity_df is not None:\n",
    "                    print(f\"  - Records: {len(similarity_df):,}\")\n",
    "                    print(f\"  - Datasets: {similarity_df['dataset'].nunique()}\")\n",
    "                    print(f\"  - Methods: {similarity_df['method'].nunique()}\")\n",
    "                    print(f\"  - Columns: {list(similarity_df.columns)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  - Error loading cache: {str(e)}\")\n",
    "    \n",
    "    # Check legacy SSIM cache\n",
    "    elif os.path.exists(ssim_cache_file):\n",
    "        size = os.path.getsize(ssim_cache_file)\n",
    "        mod_time = os.path.getmtime(ssim_cache_file)\n",
    "        mod_time_str = pd.to_datetime(mod_time, unit='s').strftime('%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        print(f\"Legacy SSIM cache file exists:\")\n",
    "        print(f\"  - Size: {size:,} bytes\")\n",
    "        print(f\"  - Modified: {mod_time_str}\")\n",
    "        \n",
    "        # Try to load and show basic info\n",
    "        try:\n",
    "            with open(ssim_cache_file, 'rb') as f:\n",
    "                cached_data = pickle.load(f)\n",
    "                ssim_df = cached_data.get('ssim_df')\n",
    "                if ssim_df is not None:\n",
    "                    print(f\"  - Records: {len(ssim_df):,}\")\n",
    "                    print(f\"  - Datasets: {ssim_df['dataset'].nunique()}\")\n",
    "                    print(f\"  - Methods: {ssim_df['method'].nunique()}\")\n",
    "                    print(f\"  - Columns: {list(ssim_df.columns)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  - Error loading cache: {str(e)}\")\n",
    "    else:\n",
    "        print(\"No similarity metrics cache files found\")\n",
    "\n",
    "# Run cache info by default\n",
    "print(\"=== CACHE INFORMATION ===\")\n",
    "list_cache_contents()\n",
    "print()\n",
    "get_cache_info()\n",
    "\n",
    "# Uncomment the line below to clear the cache if needed\n",
    "clear_analysis_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b86e3709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9559743692319546\n",
      "1.3131666666666666%\n"
     ]
    }
   ],
   "source": [
    "query_id = 40\n",
    "measure = 10\n",
    "dataset = \"soccer_exp\"\n",
    "width = 1000\n",
    "height = 600\n",
    "outFolder = \"_output_no_allocation\"\n",
    "gt_df = pd.read_csv(f\"../{outFolder}/timeQueries/m4/influx/{dataset}/run_0/query_{query_id}/{measure}.csv\")\n",
    "method_df = pd.read_csv(f\"../{outFolder}/timeCacheQueries/m4Inf/influx/{dataset}/run_0/query_{query_id}/{measure}.csv\")\n",
    "\n",
    "# Create temporary image files\n",
    "gt_img_path = os.path.join(f\"{dataset}_{query_id}_{measure}_gt.png\")\n",
    "method_img_path = os.path.join(f\"{dataset}_{query_id}_{measure}_method.png\")\n",
    "\n",
    "# Get min and max timestamps to use the same scale for both plots\n",
    "min_ts = method_df['timestamp'].min()\n",
    "max_ts = method_df['timestamp'].max()\n",
    "# Generate plots using cairo_plot\n",
    "plot(gt_df, str(measure), gt_img_path.replace('.png', ''), width, height, min_ts, max_ts)\n",
    "plot(method_df, str(measure), method_img_path.replace('.png', ''), width, height, min_ts, max_ts)\n",
    "\n",
    "# Compute SSIM between the two images\n",
    "ssim_score = compute_ssim(gt_img_path, method_img_path)\n",
    "\n",
    "px_diff = compute_pixel_difference_percentage(gt_img_path, method_img_path)\n",
    "\n",
    "print(ssim_score)\n",
    "print(f\"{px_diff}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea0788f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98ec6c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_env",
   "language": "python",
   "name": "python_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
