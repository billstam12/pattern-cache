{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f7fa06b",
   "metadata": {},
   "source": [
    "# Query Performance Comparison Across Datasets\n",
    "\n",
    "This notebook compares query performance between different methods across all available datasets. It focuses on:\n",
    "- Comparing query execution times between standard and cached queries\n",
    "- Analyzing performance by dataset and operation type\n",
    "- Visualizing performance differences with clear and concise plots\n",
    "- Aggregating results across multiple runs for more robust comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eff8e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "import re  # Add this import for regular expressions\n",
    "from pathlib import Path\n",
    "import scipy.stats as stats\n",
    "import warnings\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set publication-ready plotting style\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'serif',\n",
    "    'font.serif': ['Times', 'Times New Roman', 'Palatino', 'DejaVu Serif'],\n",
    "    'font.size': 12,\n",
    "    'axes.titlesize': 16,\n",
    "    'axes.labelsize': 14,\n",
    "    'xtick.labelsize': 11,\n",
    "    'ytick.labelsize': 11,\n",
    "    'legend.fontsize': 10,\n",
    "    'figure.figsize': [10, 6],\n",
    "    'figure.dpi': 150,\n",
    "    'savefig.dpi': 300,\n",
    "    'savefig.bbox': 'tight',\n",
    "    'savefig.pad_inches': 0.05,\n",
    "    'axes.grid': True,\n",
    "    'grid.alpha': 0.3,\n",
    "    'axes.axisbelow': True,\n",
    "    'axes.labelpad': 10\n",
    "})\n",
    "\n",
    "# Define output folders\n",
    "outFolder = \"output_no_allocation\"\n",
    "groundTruthFolder = \"output_no_allocation\"\n",
    "\n",
    "plt.rcParams['text.usetex'] = False  # Set to True only if you have LaTeX installed\n",
    "plt.rcParams['mathtext.default'] = 'regular'\n",
    "\n",
    "# Define the methods to compare\n",
    "METHODS = [\n",
    "    {\n",
    "        \"name\": \"M4-NoC\",\n",
    "        \"path\": f\"../{groundTruthFolder}/timeQueries/\",\n",
    "        \"method\": \"m4\",\n",
    "        \"database\": \"influx\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"M4$^\\\\infty$-C\",\n",
    "        \"path\": f\"../{outFolder}/timeCacheQueries/\",\n",
    "        \"method\": \"m4Inf\",\n",
    "        \"database\": \"influx\"\n",
    "    },\n",
    "    # {\n",
    "    #     \"name\": \"MinMaxCache\",\n",
    "    #     \"path\": f\"../{outFolder}/timeMinMaxCacheQueries/\",\n",
    "    #     \"method\": \"minmax\",\n",
    "    #     \"database\": \"influx\"\n",
    "    # },\n",
    "]\n",
    "\n",
    "# Create a folder for saving publication-ready figures\n",
    "FIGURES_DIR = \"../figures\"\n",
    "os.makedirs(FIGURES_DIR, exist_ok=True)\n",
    "\n",
    "# Generate a publication-ready color palette\n",
    "# Using ColorBrewer-inspired palette for better distinction in papers\n",
    "METHOD_COLORS = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
    "\n",
    "# Define a consistent style function for publication-ready plots\n",
    "def set_publication_style(ax, title=None, xlabel=None, ylabel=None, legend_title=None):\n",
    "    \"\"\"Apply consistent publication-ready styling to matplotlib axis\"\"\"\n",
    "    if title:\n",
    "        ax.set_title(title, fontweight='bold', pad=15)\n",
    "    if xlabel:\n",
    "        ax.set_xlabel(xlabel, fontweight='bold')\n",
    "    if ylabel:\n",
    "        ax.set_ylabel(ylabel, fontweight='bold')\n",
    "    \n",
    "    # Apply grid style\n",
    "    ax.grid(True, linestyle='--', alpha=0.3)\n",
    "    ax.set_axisbelow(True)\n",
    "    \n",
    "    # Style spines\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_linewidth(1.5)\n",
    "    \n",
    "    # Format legend if it exists\n",
    "    if ax.get_legend():\n",
    "        if legend_title:\n",
    "            ax.legend(title=legend_title, frameon=True, facecolor='white', \n",
    "                     framealpha=0.9, edgecolor='black')\n",
    "        else:\n",
    "            ax.legend(frameon=True, facecolor='white', framealpha=0.9, edgecolor='black')\n",
    "    \n",
    "    # Remove top and right spines\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836d9f9c",
   "metadata": {},
   "source": [
    "## Load Query Results Data\n",
    "\n",
    "Load experiment results from all available datasets, aggregating across runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5995cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_results(base_path, method, database_type, table_name):\n",
    "    \"\"\"\n",
    "    Load results from multiple experiment runs into a single dataframe\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    base_path : str\n",
    "        Base path to the queries directory\n",
    "    method : str    \n",
    "        Name of the method used (e.g., m4Inf, m4)\n",
    "    database_type : str\n",
    "        Type of database (influx, postgres, etc.)\n",
    "    table_name : str\n",
    "        Name of the database table\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame or None: Combined results from all runs\n",
    "    \"\"\"\n",
    "    path_pattern = os.path.join(base_path, method, database_type, table_name, \"run_*\", \"results.csv\")\n",
    "    csv_files = glob.glob(path_pattern)\n",
    "    \n",
    "    if not csv_files:\n",
    "        return None\n",
    "    \n",
    "    dfs = []\n",
    "    for csv_file in csv_files:\n",
    "        run_name = os.path.basename(os.path.dirname(csv_file))\n",
    "        df = pd.read_csv(csv_file)\n",
    "        df['run'] = run_name\n",
    "        df['dataset'] = table_name\n",
    "        dfs.append(df)\n",
    "    \n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Convert date columns to datetime if they exist\n",
    "    date_columns = ['from', 'to']\n",
    "    for col in date_columns:\n",
    "        if col in combined_df.columns:\n",
    "            combined_df[col] = pd.to_datetime(combined_df[col])\n",
    "            \n",
    "    # Add duration column\n",
    "    if 'from' in combined_df.columns and 'to' in combined_df.columns:\n",
    "        combined_df['duration_sec'] = (combined_df['to'] - combined_df['from']).dt.total_seconds()\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "# Function to aggregate results from multiple runs\n",
    "def aggregate_runs(df):\n",
    "    \"\"\"\n",
    "    Aggregate results from multiple runs by grouping by query characteristics\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame containing results from multiple runs\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame: Aggregated results with statistics across runs\n",
    "    \"\"\"\n",
    "    if df is None or len(df) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Group by query characteristics (not by run)\n",
    "    groupby_cols = ['dataset', 'query #', 'query_type', 'group_by', 'aggregation', 'time_interval']\n",
    "    group_cols = [col for col in groupby_cols if col in df.columns]\n",
    "    \n",
    "    # Aggregate the Time (sec) column across runs\n",
    "    agg_df = df.groupby(group_cols).agg({\n",
    "        'Time (sec)': ['mean', 'median', 'std', 'min', 'max', 'count'],\n",
    "        'run': 'nunique'  # Count number of runs\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Flatten the multi-level column names\n",
    "    agg_df.columns = [f\"{col[0]}_{col[1]}\" if col[1] else col[0] for col in agg_df.columns]\n",
    "    \n",
    "    # Rename some columns for clarity\n",
    "    agg_df = agg_df.rename(columns={\n",
    "        'Time (sec)_mean': 'Time (sec)', \n",
    "        'Time (sec)_count': 'query_count',\n",
    "        'run_nunique': 'run_count'\n",
    "    })\n",
    "    \n",
    "    return agg_df\n",
    "\n",
    "# Operation type mapping for better readability\n",
    "def get_operation_type_mapping():\n",
    "    return {\n",
    "        'P': 'Pan',\n",
    "        'ZI': 'Zoom In',\n",
    "        'ZO': 'Zoom Out',\n",
    "        'R': 'Resize',\n",
    "        'MC': 'Measure Change',\n",
    "        'PD': 'Pattern Detection',\n",
    "        'NaN': 'Initial Query'\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de14474c",
   "metadata": {},
   "source": [
    "## Find All Available Datasets and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b30cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, find all available datasets by looking at directories\n",
    "def find_datasets(method_info):\n",
    "    base_path = method_info[\"path\"]\n",
    "    method = method_info[\"method\"]\n",
    "    database = method_info[\"database\"]\n",
    "    \n",
    "    # Find all dataset directories\n",
    "    dataset_pattern = os.path.join(base_path, method, database, \"*\")\n",
    "    datasets = []\n",
    "    \n",
    "    for dataset_dir in glob.glob(dataset_pattern):\n",
    "        if os.path.isdir(dataset_dir):\n",
    "            dataset_name = os.path.basename(dataset_dir)\n",
    "            datasets.append(dataset_name)\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "# Get unique datasets from all methods\n",
    "all_datasets = set()\n",
    "for method in METHODS:\n",
    "    datasets = find_datasets(method)\n",
    "    all_datasets.update(datasets)\n",
    "\n",
    "all_datasets = sorted(list(all_datasets))\n",
    "print(f\"Found {len(all_datasets)} datasets: {all_datasets}\")\n",
    "\n",
    "# Load data for each method and dataset\n",
    "raw_results_by_dataset = {}  # Store raw results\n",
    "results_by_dataset = {}      # Store aggregated results\n",
    "all_results = []             # Store all aggregated results\n",
    "\n",
    "for dataset in all_datasets:\n",
    "    print(f\"\\nLoading data for dataset: {dataset}\")\n",
    "    dataset_results = []\n",
    "    dataset_raw_results = []\n",
    "    \n",
    "    for i, method in enumerate(METHODS):\n",
    "        print(f\"  Loading {method['name']}...\")\n",
    "        \n",
    "        df = load_results(\n",
    "            base_path=method['path'],\n",
    "            method=method['method'],\n",
    "            database_type=method['database'],\n",
    "            table_name=dataset\n",
    "        )\n",
    "        \n",
    "        if df is not None and not df.empty:\n",
    "            # Add method name and color index\n",
    "            df['method'] = method['name']\n",
    "            df['method_idx'] = i\n",
    "            \n",
    "            # Add readable operation type\n",
    "            op_type_map = get_operation_type_mapping()\n",
    "            df['operation'] = df.apply(\n",
    "                lambda row: 'Initial Query' if pd.isna(row['query_type']) else op_type_map.get(row['query_type'], row['query_type']), \n",
    "                axis=1\n",
    "            )\n",
    "            \n",
    "            # Store raw results first\n",
    "            dataset_raw_results.append(df)\n",
    "            \n",
    "            # Aggregate results across runs\n",
    "            agg_df = aggregate_runs(df)\n",
    "            if agg_df is not None:\n",
    "                # Add method name and operation type to aggregated data\n",
    "                agg_df['method'] = method['name']\n",
    "                agg_df['method_idx'] = i\n",
    "                agg_df['operation'] = agg_df.apply(\n",
    "                    lambda row: 'Initial Query' if pd.isna(row['query_type']) else op_type_map.get(row['query_type'], row['query_type']), \n",
    "                    axis=1\n",
    "                )\n",
    "                \n",
    "                dataset_results.append(agg_df)\n",
    "                all_results.append(agg_df)\n",
    "                print(f\"    Loaded {len(df)} queries from {agg_df['run_count'].iloc[0]} runs, aggregated to {len(agg_df)} unique queries\")\n",
    "            else:\n",
    "                print(f\"    Error aggregating results\")\n",
    "        else:\n",
    "            print(f\"    No data found\")\n",
    "    \n",
    "    if dataset_raw_results:\n",
    "        raw_results_by_dataset[dataset] = pd.concat(dataset_raw_results, ignore_index=True)\n",
    "    \n",
    "    if dataset_results:\n",
    "        results_by_dataset[dataset] = pd.concat(dataset_results, ignore_index=True)\n",
    "\n",
    "# Combine all results into a single dataframe for overall analysis\n",
    "if all_results:\n",
    "    all_combined = pd.concat(all_results, ignore_index=True)\n",
    "    print(f\"\\nLoaded a total of {len(all_combined)} aggregated queries across {len(all_datasets)} datasets\")\n",
    "    \n",
    "    # Print run count information\n",
    "    for dataset in results_by_dataset:\n",
    "        for method in METHODS:\n",
    "            method_name = method['name']\n",
    "            method_data = results_by_dataset[dataset][results_by_dataset[dataset]['method'] == method_name]\n",
    "            if not method_data.empty:\n",
    "                run_count = method_data['run_count'].iloc[0]\n",
    "                print(f\"Dataset: {dataset}, Method: {method_name}, Runs: {run_count}\")\n",
    "else:\n",
    "    all_combined = None\n",
    "    print(\"\\nNo data was loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad3ea32",
   "metadata": {},
   "source": [
    "## Performance Comparison by Dataset\n",
    "\n",
    "Let's compare the performance across all datasets using the aggregated results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a7f730",
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_combined is not None:\n",
    "    # Create a summary table of performance by dataset and method\n",
    "    dataset_summary = all_combined.groupby(['dataset', 'method'])[['Time (sec)', 'Time (sec)_median', 'query_count', 'run_count']].agg([\n",
    "        'mean'\n",
    "    ]).reset_index()\n",
    "    \n",
    "    # Flatten the column multi-index\n",
    "    dataset_summary.columns = ['_'.join(col).strip('_') for col in dataset_summary.columns.values]\n",
    "    \n",
    "    # Rename columns for clarity\n",
    "    dataset_summary = dataset_summary.rename(columns={\n",
    "        'dataset_': 'dataset',\n",
    "        'method_': 'method',\n",
    "        'Time (sec)_mean': 'mean_time',\n",
    "        'Time (sec)_median_mean': 'median_time',\n",
    "        'query_count_mean': 'avg_query_count',\n",
    "        'run_count_mean': 'run_count'\n",
    "    })\n",
    "    \n",
    "    # Display summary table\n",
    "    print(\"Summary of Query Times by Dataset and Method:\")\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    display(dataset_summary)\n",
    "    \n",
    "    # Calculate improvement percentages\n",
    "    improvement_data = []\n",
    "    \n",
    "    for dataset in all_datasets:\n",
    "        dataset_data = dataset_summary[dataset_summary['dataset'] == dataset]\n",
    "        \n",
    "        if len(dataset_data) >= 2:  # Need at least two methods for comparison\n",
    "            methods = dataset_data['method'].unique()\n",
    "            if len(methods) >= 2:\n",
    "                baseline_method = METHODS[0]['name']\n",
    "                baseline_avg = dataset_data[dataset_data['method'] == baseline_method]['mean_time'].values[0]\n",
    "                \n",
    "                for method in methods:\n",
    "                    if method != baseline_method:\n",
    "                        method_avg = dataset_data[dataset_data['method'] == method]['mean_time'].values[0]\n",
    "                        imp_pct = ((baseline_avg - method_avg) / baseline_avg) * 100\n",
    "                        \n",
    "                        improvement_data.append({\n",
    "                            'Dataset': dataset,\n",
    "                            'Baseline': baseline_method,\n",
    "                            'Compared Method': method,\n",
    "                            'Baseline Avg (sec)': baseline_avg,\n",
    "                            'Method Avg (sec)': method_avg,\n",
    "                            'Improvement %': imp_pct,\n",
    "                            'Status': 'Faster' if imp_pct > 0 else 'Slower'\n",
    "                        })\n",
    "    \n",
    "    # Display improvement percentages\n",
    "    if improvement_data:\n",
    "        improvements_df = pd.DataFrame(improvement_data)\n",
    "        print(\"\\nPerformance Improvement Summary:\")\n",
    "        display(improvements_df.sort_values('Improvement %', ascending=False))\n",
    "        \n",
    "        # Sort datasets by improvement percentage for better visualization\n",
    "        improvements_df = improvements_df.sort_values('Improvement %')\n",
    "        \n",
    "        # Create publication-ready improvement chart\n",
    "        fig, ax = plt.subplots(figsize=(10, max(6, len(all_datasets) * 0.7)))\n",
    "        \n",
    "        # Create horizontal bars with custom colors\n",
    "        bars = ax.barh(\n",
    "            improvements_df['Dataset'], \n",
    "            improvements_df['Improvement %'],\n",
    "            color=[plt.cm.RdYlGn(0.7 * (x + 100) / 150) if x > 0 else plt.cm.RdYlGn(0.3 * (x + 100) / 100) for x in improvements_df['Improvement %']],\n",
    "            edgecolor='black',\n",
    "            linewidth=0.8\n",
    "        )\n",
    "        \n",
    "        # Add percentage labels with proper formatting\n",
    "        for bar in bars:\n",
    "            width = bar.get_width()\n",
    "            label_x_pos = width + 1 if width > 0 else width - 1\n",
    "            ha = 'left' if width > 0 else 'right'\n",
    "            value = f\"+{width:.1f}%\" if width > 0 else f\"{width:.1f}%\"\n",
    "            fontweight = 'bold' if abs(width) > 20 else 'normal'\n",
    "            \n",
    "            ax.text(label_x_pos, bar.get_y() + bar.get_height()/2, \n",
    "                   value, va='center', ha=ha, fontsize=11, fontweight=fontweight)\n",
    "        \n",
    "        # Draw vertical line at x=0\n",
    "        ax.axvline(x=0, color='black', linestyle='-', linewidth=1.5)\n",
    "        \n",
    "        # Set axis labels and title\n",
    "        ax.set_xlabel('Performance Improvement (%)')\n",
    "        ax.set_title(f'Cache Performance Impact by Dataset\\n({METHODS[1][\"name\"]} vs {METHODS[0][\"name\"]})')\n",
    "        \n",
    "        # Add explanatory text\n",
    "        ax.text(0.02, 0.02, \n",
    "                \"Positive values indicate faster performance with cache\\nNegative values indicate slower performance with cache\",\n",
    "                transform=ax.transAxes, fontsize=10, va='bottom', \n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8))\n",
    "        \n",
    "        # Format x-axis with percentage symbol\n",
    "        ax.xaxis.set_major_formatter(ticker.PercentFormatter())\n",
    "        \n",
    "        # Apply publication styling\n",
    "        set_publication_style(ax)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save figure\n",
    "        plt.savefig(os.path.join(FIGURES_DIR, \"improvement_by_dataset.pdf\"))\n",
    "        plt.savefig(os.path.join(FIGURES_DIR, \"improvement_by_dataset.png\"))\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"No data available for comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05cdfaf",
   "metadata": {},
   "source": [
    "## Performance Comparison by Operation Type\n",
    "\n",
    "Let's break down the performance by operation type for each dataset using the aggregated results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0ee8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_combined is not None:\n",
    "    # Get unique operations across all datasets\n",
    "    all_ops = sorted(all_combined['operation'].unique())\n",
    "    \n",
    "    # Create a figure with subplots - one per dataset\n",
    "    n_datasets = len(all_datasets)\n",
    "    \n",
    "    # Create publication-ready figures for each dataset separately\n",
    "    for i, dataset in enumerate(all_datasets):\n",
    "        if dataset in results_by_dataset:\n",
    "            dataset_df = results_by_dataset[dataset]\n",
    "            method_names = dataset_df['method'].unique()\n",
    "            run_count = dataset_df['run_count'].iloc[0]\n",
    "            \n",
    "            # Group data by operation type and method\n",
    "            op_perf = dataset_df.groupby(['operation', 'method'])['Time (sec)'].mean().reset_index()\n",
    "            \n",
    "            # Get operations for this dataset and sort them in a meaningful order\n",
    "            operations = op_perf['operation'].unique()\n",
    "            op_order = ['Initial Query', 'Pan', 'Zoom In', 'Zoom Out', 'Resize', 'Measure Change', 'Pattern Detection']\n",
    "            operations = sorted(operations, key=lambda x: op_order.index(x) if x in op_order else 999)\n",
    "            \n",
    "            # Create the figure\n",
    "            fig, ax = plt.subplots(figsize=(12, 7))\n",
    "            \n",
    "            # Set bar properties\n",
    "            bar_width = 0.35\n",
    "            opacity = 0.8\n",
    "            bar_positions = np.arange(len(operations))\n",
    "            method_colors = METHOD_COLORS[:len(method_names)]\n",
    "            \n",
    "            for j, method in enumerate(method_names):\n",
    "                method_data = op_perf[op_perf['method'] == method]\n",
    "                # Create a lookup dict by operation\n",
    "                method_by_op = {row['operation']: row['Time (sec)'] for _, row in method_data.iterrows()}\n",
    "                \n",
    "                # Extract values in the correct order\n",
    "                values = [method_by_op.get(op, 0) for op in operations]\n",
    "                \n",
    "                # Plot bars\n",
    "                offset = (j - len(method_names)/2 + 0.5) * bar_width\n",
    "                bars = ax.bar(\n",
    "                    bar_positions + offset, \n",
    "                    values, \n",
    "                    bar_width,\n",
    "                    color=method_colors[j], \n",
    "                    label=method,\n",
    "                    edgecolor='black',\n",
    "                    linewidth=1,\n",
    "                    alpha=opacity\n",
    "                )\n",
    "                \n",
    "                # Add value labels on top of bars\n",
    "                for k, bar in enumerate(bars):\n",
    "                    height = bar.get_height()\n",
    "                    ax.text(\n",
    "                        bar.get_x() + bar.get_width()/2,\n",
    "                        height + 0.05,\n",
    "                        f'{height:.2f}',\n",
    "                        ha='center', \n",
    "                        va='bottom',\n",
    "                        fontsize=9,\n",
    "                        rotation=0\n",
    "                    )\n",
    "            \n",
    "            # Set the x-axis labels\n",
    "            ax.set_xticks(bar_positions)\n",
    "            ax.set_xticklabels(operations, rotation=45, ha='right')\n",
    "            \n",
    "            # Set labels and title\n",
    "            title = f'Query Performance by Operation Type - {dataset}'\n",
    "            subtitle = f'Average across {run_count} runs'\n",
    "            ax.set_title(f'{title}\\n{subtitle}', pad=20)\n",
    "            ax.set_xlabel('Operation Type')\n",
    "            ax.set_ylabel('Average Time (seconds)')\n",
    "            \n",
    "            # Create a custom legend with method names and color patches\n",
    "            ax.legend(title='Query Method')\n",
    "            \n",
    "            # Apply publication styling\n",
    "            set_publication_style(ax, legend_title='Query Method')\n",
    "            \n",
    "            # Adjust layout\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save figure\n",
    "            filename_safe = dataset.replace(' ', '_').lower()\n",
    "            plt.savefig(os.path.join(FIGURES_DIR, f\"operations_{filename_safe}.pdf\"))\n",
    "            plt.savefig(os.path.join(FIGURES_DIR, f\"operations_{filename_safe}.png\"))\n",
    "            plt.show()\n",
    "    \n",
    "    # Create detailed operation comparison tables for each dataset\n",
    "    for dataset in all_datasets:\n",
    "        if dataset in results_by_dataset:\n",
    "            print(f\"\\n\\n### Performance Breakdown for Dataset: {dataset} (Averaged across {results_by_dataset[dataset]['run_count'].iloc[0]} runs)\")\n",
    "            \n",
    "            dataset_df = results_by_dataset[dataset]\n",
    "            \n",
    "            # Detailed operation type comparison including standard deviations\n",
    "            op_stats = dataset_df.pivot_table(\n",
    "                index='operation',\n",
    "                columns='method',\n",
    "                values=['Time (sec)', 'Time (sec)_std', 'query_count']\n",
    "            )\n",
    "            \n",
    "            display(op_stats)\n",
    "            \n",
    "            # Calculate improvements for this dataset by operation\n",
    "            if len(dataset_df['method'].unique()) >= 2:\n",
    "                baseline_method = METHODS[0]['name']\n",
    "                comparison_method = METHODS[1]['name']\n",
    "                \n",
    "                # Get data for comparison\n",
    "                baseline_data = dataset_df[dataset_df['method'] == baseline_method]\n",
    "                comparison_data = dataset_df[dataset_df['method'] == comparison_method]\n",
    "                \n",
    "                # Group by operation and calculate stats\n",
    "                ops_baseline = baseline_data.groupby('operation')['Time (sec)'].agg(['mean', 'count']).reset_index()\n",
    "                ops_comparison = comparison_data.groupby('operation')['Time (sec)'].agg(['mean', 'count']).reset_index()\n",
    "                \n",
    "                # Merge the data\n",
    "                ops_merged = pd.merge(ops_baseline, ops_comparison, on='operation', suffixes=('_baseline', '_comparison'))\n",
    "                \n",
    "                # Calculate improvement percentage\n",
    "                ops_merged['improvement_pct'] = ((ops_merged['mean_baseline'] - ops_merged['mean_comparison']) / \n",
    "                                              ops_merged['mean_baseline']) * 100\n",
    "                \n",
    "                # Sort by improvement percentage\n",
    "                ops_merged = ops_merged.sort_values('improvement_pct', ascending=False)\n",
    "                \n",
    "                # Display the results\n",
    "                print(f\"\\nOperation Improvements ({baseline_method} vs {comparison_method}):\")\n",
    "                display(ops_merged[['operation', 'mean_baseline', 'mean_comparison', \n",
    "                                  'improvement_pct', 'count_baseline', 'count_comparison']])\n",
    "                \n",
    "                # Create publication-ready chart for operation improvements\n",
    "                fig, ax = plt.subplots(figsize=(12, 7))\n",
    "                \n",
    "                # Sort operations by improvement percentage\n",
    "                sorted_ops = ops_merged.sort_values('improvement_pct', ascending=True)\n",
    "                \n",
    "                # Create horizontal bars with custom colors\n",
    "                bars = ax.barh(\n",
    "                    sorted_ops['operation'], \n",
    "                    sorted_ops['improvement_pct'], \n",
    "                    color=[plt.cm.RdYlGn(0.7 * (x + 100) / 150) if x > 0 else plt.cm.RdYlGn(0.3 * (x + 100) / 100) for x in sorted_ops['improvement_pct']],\n",
    "                    edgecolor='black',\n",
    "                    linewidth=1\n",
    "                )\n",
    "                \n",
    "                # Add value labels\n",
    "                for bar in bars:\n",
    "                    width = bar.get_width()\n",
    "                    label_x_pos = width + 1 if width > 0 else width - 1\n",
    "                    ha = 'left' if width > 0 else 'right'\n",
    "                    value = f\"+{width:.1f}%\" if width > 0 else f\"{width:.1f}%\"\n",
    "                    \n",
    "                    ax.text(label_x_pos, bar.get_y() + bar.get_height()/2, \n",
    "                           value, va='center', ha=ha, fontsize=11)\n",
    "                \n",
    "                # Add vertical line at x=0\n",
    "                ax.axvline(x=0, color='black', linestyle='-', linewidth=1.5)\n",
    "                \n",
    "                # Format axes\n",
    "                ax.set_title(f'Performance Impact by Operation Type - {dataset}\\n({comparison_method} vs {baseline_method})', pad=20)\n",
    "                ax.set_xlabel('Improvement (%)')\n",
    "                ax.set_ylabel('Operation Type')\n",
    "                ax.xaxis.set_major_formatter(ticker.PercentFormatter())\n",
    "                \n",
    "                # Apply publication styling\n",
    "                set_publication_style(ax)\n",
    "                \n",
    "                # Save figure\n",
    "                filename_safe = dataset.replace(' ', '_').lower()\n",
    "                plt.savefig(os.path.join(FIGURES_DIR, f\"operation_improvements_{filename_safe}.pdf\"))\n",
    "                plt.savefig(os.path.join(FIGURES_DIR, f\"operation_improvements_{filename_safe}.png\"))\n",
    "                plt.show()\n",
    "else:\n",
    "    print(\"No data available for comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa46955",
   "metadata": {},
   "source": [
    "## Query Execution Time Evolution\n",
    "\n",
    "Let's visualize how query times evolve across the sequence of operations, highlighting pattern detection queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff42579",
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_combined is not None:\n",
    "    # Plot time series for each dataset\n",
    "    for dataset in all_datasets:\n",
    "        if dataset in results_by_dataset:\n",
    "            dataset_df = results_by_dataset[dataset]\n",
    "            run_count = dataset_df['run_count'].iloc[0]\n",
    "            \n",
    "            # Create a publication-ready time series plot\n",
    "            fig, ax = plt.subplots(figsize=(12, 7))\n",
    "            \n",
    "            methods = dataset_df['method'].unique()\n",
    "            \n",
    "            # Define markers and line styles by operation type\n",
    "            markers = {'Pattern Detection': '*', 'Other': 'o'}\n",
    "            \n",
    "            # Create a variable to track the max y-value for annotation positioning\n",
    "            max_y = 0\n",
    "            \n",
    "            # Plot each method\n",
    "            for i, method in enumerate(methods):\n",
    "                method_data = dataset_df[dataset_df['method'] == method].sort_values('query #')\n",
    "                \n",
    "                # Color for this method\n",
    "                color = METHOD_COLORS[i]\n",
    "                \n",
    "                # Plot standard queries with error bars\n",
    "                line = ax.errorbar(\n",
    "                    method_data['query #'], \n",
    "                    method_data['Time (sec)'],\n",
    "                    yerr=method_data['Time (sec)_std'],\n",
    "                    label=method,\n",
    "                    marker=markers['Other'], \n",
    "                    markersize=7, \n",
    "                    alpha=0.9,\n",
    "                    color=color, \n",
    "                    linestyle='-', \n",
    "                    linewidth=2,\n",
    "                    capsize=4,\n",
    "                    capthick=1,\n",
    "                    elinewidth=1\n",
    "                )\n",
    "                \n",
    "                # Update max y-value for annotations\n",
    "                max_y = max(max_y, (method_data['Time (sec)']).max())\n",
    "                                \n",
    "            # Add highlighting for pattern detection queries\n",
    "            pattern_queries = dataset_df[dataset_df['operation'] == 'Pattern Detection']['query #'].unique()\n",
    "            for query_num in pattern_queries:\n",
    "                ax.axvline(x=query_num, color='lightgray', linestyle='--', alpha=0.5, zorder=0)\n",
    "            \n",
    "            # Set proper titles and labels\n",
    "            title = f'Query Execution Time Evolution - {dataset}'\n",
    "            subtitle = f'Average of {run_count} runs with standard deviation'\n",
    "            ax.set_title(f'{title}\\n{subtitle}', pad=20)\n",
    "            ax.set_xlabel('Query Sequence Number')\n",
    "            ax.set_ylabel('Execution Time (seconds)')\n",
    "            \n",
    "            # Format x-axis as integers\n",
    "            ax.xaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n",
    "            \n",
    "            # Apply publication styling\n",
    "            set_publication_style(ax, legend_title='Query Method')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save figure\n",
    "            filename_safe = dataset.replace(' ', '_').lower()\n",
    "            plt.savefig(os.path.join(FIGURES_DIR, f\"time_evolution_{filename_safe}.pdf\"))\n",
    "            plt.savefig(os.path.join(FIGURES_DIR, f\"time_evolution_{filename_safe}.png\"))\n",
    "            plt.show()\n",
    "            \n",
    "            # Create a separate plot showing only pattern detection queries if they exist\n",
    "            pattern_methods = []\n",
    "            for method in methods:\n",
    "                method_data = dataset_df[dataset_df['method'] == method]\n",
    "                pd_data = method_data[method_data['operation'] == 'Pattern Detection']\n",
    "                if not pd_data.empty:\n",
    "                    pattern_methods.append(method)\n",
    "            \n",
    "            if pattern_methods:\n",
    "                fig, ax = plt.subplots(figsize=(10, 6))\n",
    "                \n",
    "                for i, method in enumerate(pattern_methods):\n",
    "                    method_data = dataset_df[dataset_df['method'] == method]\n",
    "                    pd_data = method_data[method_data['operation'] == 'Pattern Detection'].sort_values('query #')\n",
    "                    \n",
    "                    ax.errorbar(\n",
    "                        pd_data['query #'], \n",
    "                        pd_data['Time (sec)'],\n",
    "                        yerr=pd_data['Time (sec)_std'],\n",
    "                        label=method, \n",
    "                        marker=markers['Other'],\n",
    "                        markersize=7,\n",
    "                        color=METHOD_COLORS[i],\n",
    "                        capsize=4,\n",
    "                        capthick=1.5,\n",
    "                        elinewidth=1.5,\n",
    "                        linewidth=0\n",
    "                    )\n",
    "                \n",
    "                # Set titles and labels\n",
    "                title = f'Pattern Detection Query Performance - {dataset}'\n",
    "                subtitle = f'Average of {run_count} runs'\n",
    "                ax.set_title(f'{title}\\n{subtitle}', pad=20)\n",
    "                ax.set_xlabel('Query Sequence Number')\n",
    "                ax.set_ylabel('Execution Time (seconds)')\n",
    "                \n",
    "                # Format x-axis as integers\n",
    "                ax.xaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n",
    "                \n",
    "                # Apply publication styling\n",
    "                set_publication_style(ax, legend_title='Query Method')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                \n",
    "                # Save figure\n",
    "                plt.savefig(os.path.join(FIGURES_DIR, f\"pattern_detection_{filename_safe}.pdf\"))\n",
    "                plt.savefig(os.path.join(FIGURES_DIR, f\"pattern_detection_{filename_safe}.png\"))\n",
    "                plt.show()\n",
    "else:\n",
    "    print(\"No data available for plotting time evolution.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0618a68f",
   "metadata": {},
   "source": [
    "## Performance Distribution Analysis\n",
    "\n",
    "Compare the distribution of query times between methods for each dataset using aggregated statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95042bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_combined is not None:\n",
    "   \n",
    "    # We'll use raw_results_by_dataset to get the minimum value for each query\n",
    "    for dataset in all_datasets:\n",
    "        if dataset in raw_results_by_dataset:\n",
    "            dataset_df = raw_results_by_dataset[dataset]\n",
    "            run_count = len(dataset_df['run'].unique())\n",
    "            \n",
    "            # Create publication-ready chart for performance distribution\n",
    "            fig, ax = plt.subplots(figsize=(10, 7))\n",
    "            \n",
    "            methods = dataset_df['method'].unique()\n",
    "            positions = np.arange(len(methods))\n",
    "            width = 0.6\n",
    "            \n",
    "            # Create a more sophisticated boxplot-like visualization using minimum times\n",
    "            for i, method in enumerate(methods):\n",
    "                # Get data for this method\n",
    "                method_data = dataset_df[dataset_df['method'] == method]\n",
    "                \n",
    "                # Calculate the minimum time for each unique query across all runs\n",
    "                min_times_by_query = method_data.groupby(['query #'])['Time (sec)'].min().reset_index()\n",
    "                \n",
    "                # Calculate statistics on these minimum times\n",
    "                mean_val = min_times_by_query['Time (sec)'].mean()\n",
    "                median_val = min_times_by_query['Time (sec)'].median()\n",
    "                min_val = min_times_by_query['Time (sec)'].min()\n",
    "                max_val = min_times_by_query['Time (sec)'].max()\n",
    "                std_val = min_times_by_query['Time (sec)'].std()\n",
    "                \n",
    "                # Draw box - Ensure box doesn't go below zero\n",
    "                box_bottom = max(0, median_val - std_val/2)  # Use max() to prevent negative values\n",
    "                box_height = std_val\n",
    "                box = plt.Rectangle(\n",
    "                    (i-width/2, box_bottom),\n",
    "                    width, box_height,\n",
    "                    alpha=0.7,\n",
    "                    facecolor=METHOD_COLORS[i % len(METHOD_COLORS)],\n",
    "                    edgecolor='black',\n",
    "                    linewidth=1.5\n",
    "                )\n",
    "                ax.add_patch(box)\n",
    "                \n",
    "                # Draw median line\n",
    "                ax.plot([i-width/2, i+width/2], [median_val, median_val], \n",
    "                        color='white', linewidth=2.5, solid_capstyle='round')\n",
    "                ax.plot([i-width/2, i+width/2], [median_val, median_val], \n",
    "                        color='black', linewidth=1.5, solid_capstyle='round')\n",
    "                \n",
    "                # Draw whiskers (min-max) - Ensure whiskers don't go below zero\n",
    "                ax.plot([i, i], [max(0, min_val), box_bottom], \n",
    "                        color='black', linewidth=1.5, linestyle='-')\n",
    "                ax.plot([i, i], [box_bottom + box_height, max_val], \n",
    "                        color='black', linewidth=1.5, linestyle='-')\n",
    "                \n",
    "                # Draw caps on whiskers\n",
    "                whisker_width = width / 4\n",
    "                ax.plot([i-whisker_width, i+whisker_width], [max(0, min_val), max(0, min_val)], \n",
    "                        color='black', linewidth=1.5)\n",
    "                ax.plot([i-whisker_width, i+whisker_width], [max_val, max_val], \n",
    "                        color='black', linewidth=1.5)\n",
    "                \n",
    "                # Draw mean point\n",
    "                ax.plot(i, mean_val, 'o', color='white', markersize=8)\n",
    "                ax.plot(i, mean_val, 'o', color='black', markersize=6)\n",
    "                \n",
    "                # Add annotation with statistics\n",
    "                stats_text = (\n",
    "                    f\"n={len(min_times_by_query)}\\n\"\n",
    "                    f\"mean={mean_val:.2f}s\\n\"\n",
    "                    f\"median={median_val:.2f}s\\n\"\n",
    "                    f\"std={std_val:.2f}\\n\"\n",
    "                    f\"min={min_val:.2f}s\\n\"\n",
    "                    f\"max={max_val:.2f}s\"\n",
    "                )\n",
    "                \n",
    "                # Improved stats text positioning strategy\n",
    "                text_x = i + width * 0.7  # Position to the right side of the box\n",
    "                text_y = (box_bottom + box_height + max_val) / 2  # Middle between box top and max whisker\n",
    "                \n",
    "                # Alternative positioning based on dataset characteristics\n",
    "                if max_val > mean_val * 3:  # If we have extreme outliers\n",
    "                    text_y = median_val + std_val  # Place near the upper part of the box\n",
    "                \n",
    "                # Ensure text is always inside the plot area\n",
    "                y_min, y_max = ax.get_ylim()\n",
    "                text_y = min(max(text_y, y_min + (y_max - y_min) * 0.15), y_max * 0.85)\n",
    "                \n",
    "                # Add connecting line from box to annotation\n",
    "                ax.annotate(\n",
    "                    stats_text,\n",
    "                    xy=(i, median_val),  # Start from the median line\n",
    "                    xytext=(text_x, text_y),  # End at the text position\n",
    "                    textcoords=\"data\",\n",
    "                    ha='left',\n",
    "                    va='center',\n",
    "                    fontsize=9,\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.5\", fc=\"white\", ec=\"gray\", alpha=0.9),\n",
    "                    arrowprops=dict(arrowstyle=\"-\", color=\"gray\", connectionstyle=\"arc3,rad=0.3\")\n",
    "                )\n",
    "            \n",
    "            # Set axis properties\n",
    "            ax.set_xticks(positions)\n",
    "            ax.set_xticklabels(methods, fontsize=12)\n",
    "            \n",
    "            # Set titles and labels\n",
    "            title = f'Query Time Distribution - {dataset}'\n",
    "            subtitle = f'Using minimum query times across {run_count} runs'\n",
    "            ax.set_title(f'{title}\\n{subtitle}', pad=20)\n",
    "            ax.set_xlabel('Query Method')\n",
    "            ax.set_ylabel('Execution Time (seconds)')\n",
    "            \n",
    "            # Plot actual data points with jitter for better visibility\n",
    "            for i, method in enumerate(methods):\n",
    "                # Get data for this method\n",
    "                method_data = dataset_df[dataset_df['method'] == method]\n",
    "                \n",
    "                # Get minimum times for each query across runs\n",
    "                min_times_by_query = method_data.groupby(['query #'])['Time (sec)'].min().values\n",
    "                \n",
    "                # Create jitter for better point separation\n",
    "                jitter = np.random.uniform(-width/3, width/3, size=len(min_times_by_query))\n",
    "                \n",
    "                # Plot individual points with semi-transparency\n",
    "                ax.scatter(\n",
    "                    [i + j for j in jitter], \n",
    "                    min_times_by_query,\n",
    "                    s=30,\n",
    "                    alpha=0.5,\n",
    "                    color=METHOD_COLORS[i % len(METHOD_COLORS)],\n",
    "                    edgecolor='black',\n",
    "                    linewidth=0.5,\n",
    "                    zorder=3\n",
    "                )\n",
    "            \n",
    "            # Apply publication styling but without legend\n",
    "            set_publication_style(ax)\n",
    "            \n",
    "            # Adjust y-axis range\n",
    "            y_min, y_max = ax.get_ylim()\n",
    "            margin = (y_max - y_min) * 0.05\n",
    "            ax.set_ylim(y_min - margin, y_max + margin)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save figure\n",
    "            filename_safe = dataset.replace(' ', '_').lower()\n",
    "            plt.savefig(os.path.join(FIGURES_DIR, f\"min_time_distribution_{filename_safe}.pdf\"))\n",
    "            plt.savefig(os.path.join(FIGURES_DIR, f\"min_time_distribution_{filename_safe}.png\"))\n",
    "            plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"No data available for distribution analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a8f4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "def extract_patterns(filepath):\n",
    "    # Extract main [start to end] intervals per pattern\n",
    "    with open(filepath, \"r\") as f:\n",
    "        content = f.read()\n",
    "    pattern = re.compile(r\"Match #\\d+: \\[(\\d+) to (\\d+)\\]\")\n",
    "    return set((int(m.group(1)), int(m.group(2))) for m in pattern.finditer(content))\n",
    "\n",
    "def compute_f1(gt_file, pred_file):\n",
    "    gt_patterns = extract_patterns(gt_file)\n",
    "    pred_patterns = extract_patterns(pred_file)\n",
    "    # For F1, construct binary indicator vectors over union of all intervals\n",
    "    all_patterns = sorted(gt_patterns | pred_patterns)\n",
    "    gt_labels = [1 if p in gt_patterns else 0 for p in all_patterns]\n",
    "    pred_labels = [1 if p in pred_patterns else 0 for p in all_patterns]\n",
    "    precision = precision_score(gt_labels, pred_labels)\n",
    "    recall = recall_score(gt_labels, pred_labels)\n",
    "    f1 = f1_score(gt_labels, pred_labels)\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1 Score:  {f1:.4f}\")\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b141a520",
   "metadata": {},
   "source": [
    "## Pattern Detection Accuracy Comparison\n",
    "\n",
    "Compare the accuracy of pattern detection methods against the ground truth using the F1 score metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ceb1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "# Function to find ground truth pattern match files\n",
    "def find_ground_truth_files(db, dataset):\n",
    "    \"\"\"\n",
    "    Find all ground truth pattern match log files\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    base_dir : str\n",
    "        Base directory to search for ground truth files\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    list: List of ground truth file information\n",
    "    \"\"\" \n",
    "    # Get the absolute path to the pattern_matches directory\n",
    "    pattern_dir = os.path.join(\"..\", f\"{outFolder}/pattern_matches/ground_truth/{db}/{dataset}\")\n",
    "    \n",
    "    # Check if directory exists\n",
    "    if not os.path.exists(pattern_dir):\n",
    "        print(f\"Directory '{pattern_dir}' does not exist.\")\n",
    "        return []\n",
    "    \n",
    "    # Find all ground truth log files\n",
    "    gt_pattern = os.path.join(pattern_dir, \"*.log\")\n",
    "    gt_files = glob.glob(gt_pattern)\n",
    "    \n",
    "    if not gt_files:\n",
    "        print(f\"No ground truth files found in '{pattern_dir}'.\")\n",
    "        return []\n",
    "    \n",
    "    # Parse ground truth files information\n",
    "    gt_info = []\n",
    "    method_pattern = re.compile(r\"(.+?)_(\\d+)_(\\d+)_(.+?)\\.log$\")\n",
    "    \n",
    "    for file_path in gt_files:\n",
    "        filename = os.path.basename(file_path)\n",
    "        match = method_pattern.match(filename)\n",
    "        \n",
    "        if match:\n",
    "            start_ts = match.group(1)\n",
    "            end_ts = match.group(2)\n",
    "            measure = match.group(3)\n",
    "            time_unit = match.group(4)\n",
    "\n",
    "            gt_info.append({\n",
    "                'path': file_path,\n",
    "                'database': db,\n",
    "                'dataset': dataset,\n",
    "                'start_ts': start_ts,\n",
    "                'end_ts': end_ts,\n",
    "                'measure': measure,\n",
    "                'time_unit': time_unit,\n",
    "                'filename': filename\n",
    "            })\n",
    "    \n",
    "    print(f\"Found {len(gt_info)} ground truth files.\")\n",
    "    return gt_info\n",
    "\n",
    "# Function to find corresponding method files for a ground truth file\n",
    "def find_method_file(gt_info, method, base_dir=\"pattern_matches\"):\n",
    "    \"\"\"\n",
    "    Find method files that correspond to a ground truth file\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gt_info : dict\n",
    "        Ground truth file information\n",
    "    method : string\n",
    "        List of method information dictionaries\n",
    "    base_dir : str\n",
    "        Base directory to search for method files\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict: Dictionary of method files\n",
    "    \"\"\"\n",
    "    method_files = {}\n",
    "    \n",
    "    method_name = method[\"method\"]\n",
    "    # Construct a pattern to match files for this method and dataset\n",
    "    method_pattern = f\"{method_name}/{gt_info['database']}/{gt_info['dataset']}/{gt_info['start_ts']}_{gt_info['end_ts']}_{gt_info['measure']}_{gt_info['time_unit']}.log\"\n",
    "    method_path = os.path.join(\"..\", f\"{outFolder}/{base_dir}\", method_pattern)\n",
    "\n",
    "    if os.path.exists(method_path):\n",
    "        return {\n",
    "            'path': method_path,\n",
    "            'method_name': method_name,\n",
    "            'display_name': method[\"name\"]\n",
    "        }\n",
    "    return None;\n",
    "\n",
    "# Compare pattern detection accuracy for all methods against ground truth\n",
    "def compare_pattern_detection(methods, base_dir=\"pattern_matches\"):\n",
    "    \"\"\"\n",
    "    Compare pattern detection accuracy for all methods against ground truth\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    methods : list\n",
    "        List of method information dictionaries\n",
    "    base_dir : str\n",
    "        Base directory containing pattern match files\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame: DataFrame containing accuracy metrics\n",
    "    \"\"\"\n",
    " \n",
    "    results = []\n",
    "    \n",
    "    # For each ground truth file, find corresponding method files and compare\n",
    "    for i, method in enumerate(METHODS):\n",
    "        if(i == 0): continue  # Skip the first method (baseline)\n",
    "        for dataset in all_datasets:\n",
    "            gt_files = find_ground_truth_files(method['database'], dataset)\n",
    "\n",
    "            for gt_file in gt_files:\n",
    "                # Find method files for this ground truth\n",
    "                method_file = find_method_file(gt_file, method, base_dir)\n",
    "                if not method_file:\n",
    "                    print(f\"No method file found for method {method['name']} and dataset {dataset}\")\n",
    "                    continue\n",
    "                # Load ground truth patterns\n",
    "                gt_path = gt_file['path']\n",
    "                try:\n",
    "                    gt_patterns = extract_patterns(gt_path)\n",
    "                    print(f\"Ground truth: {os.path.basename(gt_path)}\")\n",
    "                    print(f\"Found {len(gt_patterns)} ground truth patterns\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading ground truth file: {str(e)}\")\n",
    "                    continue\n",
    "                \n",
    "                # Compare each method against ground truth\n",
    "                pred_path = method_file['path']\n",
    "                display_name = method_file['display_name']\n",
    "                \n",
    "                print(f\"\\nComparing {display_name} vs ground truth:\")\n",
    "                print(f\"Method file: {os.path.basename(pred_path)}\")\n",
    "                \n",
    "                try:\n",
    "                    # Load predicted patterns\n",
    "                    pred_patterns = extract_patterns(pred_path)\n",
    "                    print(f\"Found {len(pred_patterns)} predicted patterns\")\n",
    "                    \n",
    "                    # Calculate metrics\n",
    "                    all_patterns = sorted(gt_patterns | pred_patterns)\n",
    "                    gt_labels = [1 if p in gt_patterns else 0 for p in all_patterns]\n",
    "                    pred_labels = [1 if p in pred_patterns else 0 for p in all_patterns]\n",
    "                    \n",
    "                    # Handle edge cases\n",
    "                    if sum(gt_labels) == 0 and sum(pred_labels) == 0:\n",
    "                        precision = 1.0  # Both agree on no patterns\n",
    "                        recall = 1.0\n",
    "                        f1 = 1.0\n",
    "                    elif sum(pred_labels) == 0:\n",
    "                        precision = 0.0  # No predictions but ground truth has patterns\n",
    "                        recall = 0.0\n",
    "                        f1 = 0.0\n",
    "                    elif sum(gt_labels) == 0:\n",
    "                        precision = 0.0  # Predictions exist but no ground truth patterns\n",
    "                        recall = 0.0\n",
    "                        f1 = 0.0\n",
    "                    else:\n",
    "                        precision = precision_score(gt_labels, pred_labels)\n",
    "                        recall = recall_score(gt_labels, pred_labels)\n",
    "                        f1 = f1_score(gt_labels, pred_labels)\n",
    "                    \n",
    "                    # Count number of patterns\n",
    "                    num_gt_patterns = len(gt_patterns)\n",
    "                    num_pred_patterns = len(pred_patterns)\n",
    "                    num_correct_patterns = sum(1 for p in pred_patterns if p in gt_patterns)\n",
    "                    \n",
    "                    results.append({\n",
    "                        'dataset': dataset,\n",
    "                        'method': display_name,\n",
    "                        'method_id': method_name,\n",
    "                        'measure': gt_file['measure'],\n",
    "                        'time_unit': gt_file['time_unit'],\n",
    "                        'precision': precision,\n",
    "                        'recall': recall,\n",
    "                        'f1': f1,\n",
    "                        'gt_patterns': num_gt_patterns,\n",
    "                        'pred_patterns': num_pred_patterns,\n",
    "                        'correct_patterns': num_correct_patterns\n",
    "                    })\n",
    "                    \n",
    "                    print(f\"Precision:  {precision:.4f}\")\n",
    "                    print(f\"Recall:     {recall:.4f}\")\n",
    "                    print(f\"F1 Score:   {f1:.4f}\")\n",
    "                    print(f\"GT Patterns: {num_gt_patterns}, Predicted: {num_pred_patterns}, Correct: {num_correct_patterns}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error comparing {display_name} to ground truth: {str(e)}\")\n",
    "    \n",
    "    # Convert results to DataFrame\n",
    "    if results:\n",
    "        results_df = pd.DataFrame(results)\n",
    "        return results_df\n",
    "    else:\n",
    "        print(\"No comparison results generated.\")\n",
    "        return None\n",
    "\n",
    "# Run the pattern detection accuracy comparison\n",
    "print(\"Starting pattern detection accuracy comparison...\")\n",
    "accuracy_results = compare_pattern_detection(METHODS, \"pattern_matches\")\n",
    "\n",
    "if accuracy_results is not None:\n",
    "    print(\"\\nSummary of pattern detection accuracy:\")\n",
    "    display(accuracy_results)\n",
    "    \n",
    "    # Create visualizations if we have results\n",
    "    if not accuracy_results.empty:\n",
    "        # Plot F1 scores by dataset and method\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Group by dataset and method to get mean F1 scores\n",
    "        summary = accuracy_results.groupby(['dataset', 'method'])['f1'].mean().reset_index()\n",
    "        \n",
    "        # # Create a pivot table for better visualization\n",
    "        # pivot_data = summary.pivot(index='dataset', columns='method', values='f1')\n",
    "        \n",
    "        # # Plot heatmap\n",
    "        # ax = sns.heatmap(pivot_data, annot=True, fmt=\".3f\", cmap=\"YlGnBu\", \n",
    "        #                vmin=0, vmax=1, linewidths=0.5)\n",
    "        # plt.title(\"Pattern Detection F1 Score by Dataset and Method\", pad=20)\n",
    "        # plt.tight_layout()\n",
    "        \n",
    "        # # Save figure\n",
    "        # plt.savefig(os.path.join(FIGURES_DIR, \"pattern_detection_f1_scores.pdf\"))\n",
    "        # plt.savefig(os.path.join(FIGURES_DIR, \"pattern_detection_f1_scores.png\"))\n",
    "        # plt.show()\n",
    "        \n",
    "        # Create bar charts for individual metrics\n",
    "        metrics = ['precision', 'recall', 'f1']\n",
    "        fig, axes = plt.subplots(len(metrics), 1, figsize=(12, 15))\n",
    "        \n",
    "        for i, metric in enumerate(metrics):\n",
    "            # Group by dataset and method\n",
    "            summary = accuracy_results.groupby(['dataset', 'method'])[metric].mean().reset_index()\n",
    "            \n",
    "            # Create the grouped bar chart\n",
    "            sns.barplot(data=summary, x='dataset', y=metric, hue='method', ax=axes[i])\n",
    "            \n",
    "            # Set titles and labels\n",
    "            axes[i].set_title(f\"Pattern Detection {metric.capitalize()} by Dataset and Method\", pad=10)\n",
    "            axes[i].set_xlabel('Dataset')\n",
    "            axes[i].set_ylabel(metric.capitalize())\n",
    "            \n",
    "            # Apply publication styling\n",
    "            set_publication_style(axes[i], legend_title='Method')\n",
    "            \n",
    "            # Adjust y-axis limits\n",
    "            axes[i].set_ylim(0, 1.05)\n",
    "            \n",
    "            # Add text labels above bars\n",
    "            for p in axes[i].patches:\n",
    "                axes[i].annotate(f\"{p.get_height():.3f}\", \n",
    "                              (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                              ha = 'center', va = 'bottom', fontsize=8)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save figure\n",
    "        plt.savefig(os.path.join(FIGURES_DIR, \"pattern_detection_metrics.pdf\"))\n",
    "        plt.savefig(os.path.join(FIGURES_DIR, \"pattern_detection_metrics.png\"))\n",
    "        plt.show()\n",
    "        \n",
    "        # Create a summary table showing overall metrics by method\n",
    "        method_summary = accuracy_results.groupby('method').agg({\n",
    "            'precision': ['mean', 'std'],\n",
    "            'recall': ['mean', 'std'],\n",
    "            'f1': ['mean', 'std'],\n",
    "            'dataset': 'nunique'\n",
    "        }).reset_index()\n",
    "        \n",
    "        # Flatten the multi-level column names\n",
    "        method_summary.columns = ['_'.join(col).strip('_') for col in method_summary.columns]\n",
    "        \n",
    "        # Rename columns for clarity\n",
    "        method_summary = method_summary.rename(columns={\n",
    "            'method_': 'method',\n",
    "            'precision_mean': 'avg_precision',\n",
    "            'precision_std': 'std_precision',\n",
    "            'recall_mean': 'avg_recall',\n",
    "            'recall_std': 'std_recall',\n",
    "            'f1_mean': 'avg_f1',\n",
    "            'f1_std': 'std_f1',\n",
    "            'dataset_nunique': 'num_datasets'\n",
    "        })\n",
    "else:\n",
    "    print(\"No pattern match accuracy results available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1492b519",
   "metadata": {},
   "source": [
    "## Visual Similarity Comparison between Query Results\n",
    "\n",
    "Compare the visual similarity of query results between different methods and the ground truth (M4-NoC) using the Structural Similarity Index Measure (SSIM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dd8314",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import re\n",
    "import tempfile\n",
    "import shutil\n",
    "from PIL import Image\n",
    "\n",
    "# Import cairo_plot module for plotting time series\n",
    "sys.path.append(\"..\")\n",
    "from cairo_plot import plot, compute_ssim\n",
    "\n",
    "# Function to find all query CSV files for a specific dataset and method\n",
    "def find_query_csvs(base_dir, method, database, dataset):\n",
    "    \"\"\"\n",
    "    Find all query result CSV files for a specific dataset and method.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    base_dir : str\n",
    "        Base directory path\n",
    "    method : str\n",
    "        Method name (e.g., 'm4', 'm4Inf')\n",
    "    database : str\n",
    "        Database name (e.g., 'influx')\n",
    "    dataset : str\n",
    "        Dataset name\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    list: List of dictionaries containing query information\n",
    "    \"\"\"\n",
    "    query_dir = os.path.join(base_dir, method, database, dataset)\n",
    "    query_files = []\n",
    "    \n",
    "    # Check if the directory exists\n",
    "    if not os.path.exists(query_dir):\n",
    "        print(f\"Directory not found: {query_dir}\")\n",
    "        return query_files\n",
    "    \n",
    "    # Find all run directories\n",
    "    run_dirs = glob.glob(os.path.join(query_dir, \"run_*\"))\n",
    "    \n",
    "    for run_dir in run_dirs:\n",
    "        run_name = os.path.basename(run_dir)\n",
    "        \n",
    "        # Find all query directories within this run\n",
    "        query_dirs = glob.glob(os.path.join(run_dir, \"query_*\"))\n",
    "        \n",
    "        for query_dir in query_dirs:\n",
    "            query_name = os.path.basename(query_dir)\n",
    "            query_id = int(query_name.split(\"_\")[1])\n",
    "            \n",
    "            # Find all CSV files in this query directory\n",
    "            csv_files = glob.glob(os.path.join(query_dir, \"*.csv\"))\n",
    "            \n",
    "            for csv_file in csv_files:\n",
    "                measure_id = os.path.basename(csv_file).split(\".\")[0]\n",
    "                \n",
    "                query_files.append({\n",
    "                    'path': csv_file,\n",
    "                    'run': run_name,\n",
    "                    'query': query_name,\n",
    "                    'query_id': query_id,\n",
    "                    'measure': measure_id,\n",
    "                    'dataset': dataset,\n",
    "                    'method': method\n",
    "                })\n",
    "    \n",
    "    return query_files\n",
    "\n",
    "# Function to generate plot images and compute SSIM scores\n",
    "def compute_query_ssim(datasets, methods, temp_dir):\n",
    "    \"\"\"\n",
    "    Generate plots and compute SSIM scores between methods and ground truth.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    datasets : list\n",
    "        List of dataset names\n",
    "    methods : list\n",
    "        List of method configurations\n",
    "    temp_dir : str\n",
    "        Temporary directory for storing images\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame: DataFrame containing SSIM scores\n",
    "    \"\"\"\n",
    "    ssim_results = []\n",
    "    \n",
    "    # Get the first method as ground truth\n",
    "    gt_method = methods[0]\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        print(f\"Processing dataset: {dataset}\")\n",
    "        \n",
    "        # Find all query CSVs for ground truth method\n",
    "        gt_csvs = find_query_csvs(\n",
    "            gt_method['path'], \n",
    "            gt_method['method'], \n",
    "            gt_method['database'], \n",
    "            dataset\n",
    "        )\n",
    "        \n",
    "        if not gt_csvs:\n",
    "            print(f\"No ground truth CSVs found for dataset {dataset}\")\n",
    "            continue\n",
    "        \n",
    "        # Group ground truth files by query and measure\n",
    "        gt_by_query = {}\n",
    "        for gt_csv in gt_csvs:\n",
    "            key = (gt_csv['query_id'], gt_csv['measure'])\n",
    "            gt_by_query[key] = gt_csv\n",
    "        \n",
    "        # For each comparison method\n",
    "        for method_idx, method in enumerate(methods[1:], 1):  # Skip the ground truth method\n",
    "            method_name = method['name']\n",
    "            print(f\"  Comparing {method_name} with ground truth\")\n",
    "            \n",
    "            # Find all query CSVs for this method\n",
    "            method_csvs = find_query_csvs(\n",
    "                method['path'], \n",
    "                method['method'], \n",
    "                method['database'], \n",
    "                dataset\n",
    "            )\n",
    "            \n",
    "            if not method_csvs:\n",
    "                print(f\"  No CSVs found for method {method_name}\")\n",
    "                continue\n",
    "            \n",
    "            # Group method files by query and measure\n",
    "            method_by_query = {}\n",
    "            for method_csv in method_csvs:\n",
    "                key = (method_csv['query_id'], method_csv['measure'])\n",
    "                method_by_query[key] = method_csv\n",
    "            \n",
    "            # Find common queries between ground truth and method\n",
    "            common_keys = set(gt_by_query.keys()) & set(method_by_query.keys())\n",
    "            print(f\"  Found {len(common_keys)} common queries to compare\")\n",
    "            \n",
    "            # For each common query, generate plots and compute SSIM\n",
    "            for query_key in common_keys:\n",
    "                query_id, measure = query_key\n",
    "                \n",
    "                gt_csv = gt_by_query[query_key]\n",
    "                method_csv = method_by_query[query_key]\n",
    "                \n",
    "                try:\n",
    "                    # Load CSV data\n",
    "                    gt_df = pd.read_csv(gt_csv['path'])\n",
    "                    method_df = pd.read_csv(method_csv['path'])\n",
    "                    \n",
    "                    # Create temporary image files\n",
    "                    gt_img_path = os.path.join(temp_dir, f\"{dataset}_{query_id}_{measure}_gt.png\")\n",
    "                    method_img_path = os.path.join(temp_dir, f\"{dataset}_{query_id}_{measure}_method{method_idx}.png\")\n",
    "                    \n",
    "                    # Get min and max timestamps to use the same scale for both plots\n",
    "                    min_ts = min(gt_df['timestamp'].min(), method_df['timestamp'].min())\n",
    "                    max_ts = max(gt_df['timestamp'].max(), method_df['timestamp'].max())\n",
    "                    \n",
    "                    # Generate plots using cairo_plot\n",
    "                    plot(gt_df, measure, gt_img_path.replace('.png', ''), 800, 400, min_ts, max_ts)\n",
    "                    plot(method_df, measure, method_img_path.replace('.png', ''), 800, 400, min_ts, max_ts)\n",
    "                    \n",
    "                    # Compute SSIM between the two images\n",
    "                    ssim_score = compute_ssim(gt_img_path, method_img_path)\n",
    "                    \n",
    "                    # Save results\n",
    "                    ssim_results.append({\n",
    "                        'dataset': dataset,\n",
    "                        'query_id': query_id,\n",
    "                        'measure': measure,\n",
    "                        'method': method_name,\n",
    "                        'ssim': ssim_score\n",
    "                    })\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"  Error processing {dataset} query {query_id} measure {measure}: {str(e)}\")\n",
    "    \n",
    "    # Convert results to DataFrame\n",
    "    if ssim_results:\n",
    "        ssim_df = pd.DataFrame(ssim_results)\n",
    "        return ssim_df\n",
    "    else:\n",
    "        print(\"No SSIM results generated.\")\n",
    "        return None\n",
    "\n",
    "# Create temporary directory for images\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "\n",
    "try:\n",
    "    print(f\"Created temporary directory: {temp_dir}\")\n",
    "    \n",
    "    # Compute SSIM scores\n",
    "    print(\"Computing SSIM scores between query results...\")\n",
    "    ssim_df = compute_query_ssim(all_datasets, METHODS, temp_dir)\n",
    "    \n",
    "    if ssim_df is not None and not ssim_df.empty:\n",
    "        # Display summary of SSIM scores\n",
    "        print(\"\\nSummary of SSIM scores by dataset and method:\")\n",
    "        ssim_summary = ssim_df.groupby(['dataset', 'method'])['ssim'].agg(['mean', 'median', 'std', 'min', 'max', 'count']).reset_index()\n",
    "        display(ssim_summary)\n",
    "        \n",
    "        # Create box plots of SSIM scores by dataset\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Create a box plot for each dataset\n",
    "        sns.boxplot(x='dataset', y='ssim', hue='method', data=ssim_df)\n",
    "        \n",
    "        plt.title('SSIM Scores between Method Results and Ground Truth (M4-NoC)', fontsize=16)\n",
    "        plt.xlabel('Dataset', fontsize=14)\n",
    "        plt.ylabel('SSIM Score', fontsize=14)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.legend(title='Method')\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Adjust y-axis to focus on the meaningful range of SSIM scores\n",
    "        plt.ylim(max(0, ssim_df['ssim'].min() - 0.05), min(1.0, ssim_df['ssim'].max() + 0.05))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(FIGURES_DIR, \"ssim_boxplots_by_dataset.pdf\"))\n",
    "        plt.savefig(os.path.join(FIGURES_DIR, \"ssim_boxplots_by_dataset.png\"))\n",
    "        plt.show()\n",
    "        \n",
    "        # Create a more detailed box plot for each dataset\n",
    "        for dataset in ssim_df['dataset'].unique():\n",
    "            dataset_df = ssim_df[ssim_df['dataset'] == dataset]\n",
    "            \n",
    "            if len(dataset_df) > 0:\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                \n",
    "                # Create box plot for this dataset\n",
    "                ax = sns.boxplot(x='method', y='ssim', data=dataset_df, palette=METHOD_COLORS[:len(dataset_df['method'].unique())])\n",
    "                \n",
    "                # Add individual points for better visualization\n",
    "                sns.stripplot(x='method', y='ssim', data=dataset_df, \n",
    "                             size=4, color='black', alpha=0.4, jitter=True)\n",
    "                \n",
    "                plt.title(f'SSIM Scores for {dataset}', fontsize=16)\n",
    "                plt.xlabel('Method', fontsize=14)\n",
    "                plt.ylabel('SSIM Score', fontsize=14)\n",
    "                plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "                \n",
    "                # Adjust y-axis focus\n",
    "                y_min = max(0, dataset_df['ssim'].min() - 0.05)\n",
    "                y_max = min(1.0, dataset_df['ssim'].max() + 0.05)\n",
    "                plt.ylim(y_min, y_max)\n",
    "                \n",
    "                # Add count information and statistics\n",
    "                for i, method in enumerate(dataset_df['method'].unique()):\n",
    "                    method_data = dataset_df[dataset_df['method'] == method]\n",
    "                    count = len(method_data)\n",
    "                    mean_ssim = method_data['ssim'].mean()\n",
    "                    median_ssim = method_data['ssim'].median()\n",
    "                    \n",
    "                    stats_text = f\"n={count}\\n={mean_ssim:.3f}\\nmedian={median_ssim:.3f}\"\n",
    "                    ax.text(i, y_min + 0.02, stats_text, horizontalalignment='center', \n",
    "                           size='small', color='black', weight='semibold')\n",
    "                \n",
    "                # Apply publication styling\n",
    "                set_publication_style(ax)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                filename_safe = dataset.replace(' ', '_').lower()\n",
    "                plt.savefig(os.path.join(FIGURES_DIR, f\"ssim_boxplot_{filename_safe}.pdf\"))\n",
    "                plt.savefig(os.path.join(FIGURES_DIR, f\"ssim_boxplot_{filename_safe}.png\"))\n",
    "                plt.show()\n",
    "        \n",
    "        # Create a heatmap of mean SSIM scores across datasets\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        # Prepare data for the heatmap\n",
    "        heatmap_data = ssim_df.groupby(['dataset', 'method'])['ssim'].mean().reset_index()\n",
    "        pivot_data = heatmap_data.pivot(index='dataset', columns='method', values='ssim')\n",
    "        \n",
    "        # Create heatmap\n",
    "        ax = sns.heatmap(pivot_data, annot=True, fmt='.3f', cmap='YlGnBu', vmin=0, vmax=1, linewidths=0.5)\n",
    "        \n",
    "        plt.title('Average SSIM Scores by Dataset and Method', fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(FIGURES_DIR, \"ssim_heatmap.pdf\"))\n",
    "        plt.savefig(os.path.join(FIGURES_DIR, \"ssim_heatmap.png\"))\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No SSIM results available for visualization.\")\n",
    "\n",
    "finally:\n",
    "    # Clean up the temporary directory\n",
    "    shutil.rmtree(temp_dir)\n",
    "    print(f\"Removed temporary directory: {temp_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ff2b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze SSIM scores by query operation type\n",
    "if ssim_df is not None and not ssim_df.empty and all_combined is not None:\n",
    "    print(\"Analyzing SSIM scores by query operation type...\")\n",
    "    \n",
    "    # Merge SSIM results with operation types from all_combined\n",
    "    query_ops = all_combined[['dataset', 'query #', 'operation']].drop_duplicates()\n",
    "    query_ops = query_ops.rename(columns={'query #': 'query_id'})\n",
    "    \n",
    "    # Merge datasets\n",
    "    ssim_with_ops = pd.merge(\n",
    "        ssim_df, \n",
    "        query_ops,\n",
    "        on=['dataset', 'query_id'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Fill missing operations (likely due to mismatches in IDs)\n",
    "    missing_ops = ssim_with_ops['operation'].isna().sum()\n",
    "    if missing_ops > 0:\n",
    "        print(f\"Warning: Could not identify operation types for {missing_ops} queries. Filling with 'Unknown'.\")\n",
    "        ssim_with_ops['operation'] = ssim_with_ops['operation'].fillna('Unknown')\n",
    "    \n",
    "    # Display summary by operation\n",
    "    print(\"\\nSummary of SSIM scores by operation type and method:\")\n",
    "    op_summary = ssim_with_ops.groupby(['operation', 'method'])['ssim'].agg(['mean', 'median', 'std', 'min', 'max', 'count']).reset_index()\n",
    "    display(op_summary)\n",
    "    \n",
    "    # Create box plots by operation\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Create a box plot for each operation type\n",
    "    sns.boxplot(x='operation', y='ssim', hue='method', data=ssim_with_ops)\n",
    "    \n",
    "    plt.title('SSIM Scores by Operation Type', fontsize=16)\n",
    "    plt.xlabel('Operation Type', fontsize=14)\n",
    "    plt.ylabel('SSIM Score', fontsize=14)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.legend(title='Method')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Adjust y-axis focus\n",
    "    plt.ylim(max(0, ssim_with_ops['ssim'].min() - 0.05), min(1.0, ssim_with_ops['ssim'].max() + 0.05))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(FIGURES_DIR, \"ssim_boxplots_by_operation.pdf\"))\n",
    "    plt.savefig(os.path.join(FIGURES_DIR, \"ssim_boxplots_by_operation.png\"))\n",
    "    plt.show()\n",
    "    \n",
    "    # Focus on Pattern Detection operations\n",
    "    if 'Pattern Detection' in ssim_with_ops['operation'].values:\n",
    "        pattern_detection = ssim_with_ops[ssim_with_ops['operation'] == 'Pattern Detection']\n",
    "        \n",
    "        if len(pattern_detection) > 0:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            \n",
    "            # Create box plot for pattern detection operations\n",
    "            sns.boxplot(x='dataset', y='ssim', hue='method', data=pattern_detection)\n",
    "            \n",
    "            plt.title('SSIM Scores for Pattern Detection Operations', fontsize=16)\n",
    "            plt.xlabel('Dataset', fontsize=14)\n",
    "            plt.ylabel('SSIM Score', fontsize=14)\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            plt.legend(title='Method')\n",
    "            plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "            \n",
    "            # Adjust y-axis focus\n",
    "            y_min = max(0, pattern_detection['ssim'].min() - 0.05)\n",
    "            y_max = min(1.0, pattern_detection['ssim'].max() + 0.05)\n",
    "            plt.ylim(y_min, y_max)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(FIGURES_DIR, \"pattern_detection_ssim.pdf\"))\n",
    "            plt.savefig(os.path.join(FIGURES_DIR, \"pattern_detection_ssim.png\"))\n",
    "            plt.show()\n",
    "            \n",
    "            # Create a summary table for pattern detection operations\n",
    "            pd_summary = pattern_detection.groupby(['dataset', 'method'])['ssim'].agg(['mean', 'median', 'std', 'count']).reset_index()\n",
    "            print(\"\\nSSIM scores for Pattern Detection operations:\")\n",
    "            display(pd_summary)\n",
    "else:\n",
    "    print(\"SSIM results or operation data not available for visualization by operation type.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_env",
   "language": "python",
   "name": "python_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
